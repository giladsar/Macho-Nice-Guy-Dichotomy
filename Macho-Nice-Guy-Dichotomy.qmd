---
title: "Macho-Nice Guy Dichotomy"
author: "Gilad Sarusi"
format: html
editor: visual
---

## HisWords repo

For our purpose we would use the same approach that Tessa E. S. Charlesworth et al. chose. Namely,the HisWords repo which is open online in [GitHub](https://github.com/williamleif/histwords?tab=readme-ov-file).

### What's going on?

HistWords is an open-source dataset and toolkit that provides word embeddings for different historical periods, trained on Google Books Ngram (and other corpora). Each decade (1800–1990) has its own 300-dimensional vector space representing the meaning of words in that era.

Think of it as a longitudinal semantic dataset — instead of people measured over time, we have words measured across decades. Each vector encodes a word’s position in the shared “meaning space” of its time.

#### What We Can Do with It?

Inspect meaning: find nearest neighbors of a word in any decade.

Compare meanings over time: compute cosine similarity between the same word across decades to track semantic drift.

Study cultural change: analyze how the associations of a word (e.g., macho, nice) evolve by decade.

Compose phrases: average vectors (e.g., macho + guy) to represent multi-word concepts.

Align embeddings: apply sequential Procrustes alignment to make spaces from different decades comparable.

### What are We Comparng?

In this project, I'll adopt an approach similar to that of Tessa and colleagues, designed to avoid the ***Alignment Problem*** inherent in diachronic word embeddings - which makes comparison of raw cosine similaritie between two embedding spaces (e.g 1910's and 1920's) invalid.

Instead of directly comparing raw cosine similarities of words across decades, I'll examine within-decade associations—for example, comparing how strongly macho and nice are each associated with good in a given decade.

The difference between these associations is then converted into a standardized effect size, which serves as the unit of comparison across time. This method allows me to trace semantic or attitudinal change without requiring the embedding spaces to be aligned.

Nevertheless, following Tessa et al.’s consideration, I note that applying *orthogonal Procrustes* rotation could still be useful as a robustness check or for future extensions involving cross-decade word trajectory analyses.

## Code

```{r}
# ds4psych demo: HistWords usage for "macho guy" vs "nice guy"
# ------------------------------------------------------------
# Packages
library(tidyverse)
library(vroom)       # fast reading of large txts
library(stringr)
library(purrr)
library(tidyr)

# --- 0) Paths (edit to your local layout) -------------------
histwords_dir <- "vectors/english_all_sgns"  # from HistWords repo
decades <- seq(1900, 1990, by = 10)          # demo range; extend as needed
warriner_path <- "Warriner_VAD.csv"          # columns: word, valence, arousal, dominance

# --- 1) Small helpers ---------------------------------------

# Read a single decade embedding file into a matrix with rownames = words
read_embeddings <- function(path) {
  # HistWords format: first line header with counts; thereafter: word + 300 dims
  raw <- vroom(path, col_names = FALSE, progress = FALSE, delim = " ")
  # Some files include the first line as counts; detect and drop if so
  if (nchar(raw$X1[1]) > 0 && !is.na(as.numeric(raw$X1[1]))) {
    # first line looks numeric -> drop it
    raw <- raw[-1, ]
  }
  words <- raw$X1
  M <- as.matrix(raw[,-1])
  rownames(M) <- words
  storage.mode(M) <- "double"
  M
}

# Safe row grab (returns NA if word missing)
vec_or_na <- function(M, token) {
  if (!is.null(M) && token %in% rownames(M)) M[token, , drop = FALSE] else NA
}

# L2-normalize rows (so cosine = dot product)
l2_normalize <- function(M) {
  norms <- sqrt(rowSums(M^2))
  norms[norms == 0] <- 1
  M / norms
}

# Compose a phrase vector by averaging available tokens (skip missing)
compose_phrase <- function(M_norm, tokens) {
  mats <- map(tokens, ~ vec_or_na(M_norm, .x))
  keep <- keep(mats, ~ !all(is.na(.x)))
  if (length(keep) == 0) return(NA)
  out <- reduce(keep, `+`) / length(keep)
  drop(out)
}

# Concept vector = mean of multiple labels (each label can be unigram/bigram)
concept_vec <- function(M_norm, label_list) {
  # label tokens: split bigrams into components else use unigram
  label_vecs <- map(label_list, function(lbl) {
    toks <- str_split(lbl, pattern = "_|\\s+", simplify = TRUE) %>% as.character() %>% tolower()
    compose_phrase(M_norm, toks)
  })
  keep <- keep(label_vecs, ~ !all(is.na(.x)))
  if (length(keep) == 0) return(NA)
  drop(reduce(keep, `+`) / length(keep))
}

# Cosine similarity of all vocab to a single vector (assumes M_norm rows are unit)
cosine_to_all <- function(M_norm, v_unit) {
  if (any(is.na(v_unit))) return(rep(NA_real_, nrow(M_norm)))
  as.numeric(M_norm %*% v_unit)  # dot product = cosine
}

# --- 2) Define concepts (labels) -----------------------------
labels_macho <- c("macho", "manly", "tough_guy", "he_man", "alpha_male", "rugged", "virile", "dominant")
labels_nice  <- c("nice_guy", "gentleman", "kind", "considerate", "courteous", "caring", "agreeable", "good_natured")

# --- 3) Load lexicon norms (Warriner valence) ----------------
warriner <- read_csv(warriner_path, show_col_types = FALSE) |>
  transmute(word = tolower(word), valence = valence)

# --- 4) Main loop over decades (tidy) ------------------------
results <- map_df(decades, function(d) {
  message("Processing decade: ", d)
  emb_path <- file.path(histwords_dir, paste0(d, ".txt"))
  if (!file.exists(emb_path)) return(tibble())

  M <- read_embeddings(emb_path)
  M_norm <- l2_normalize(M)

  v_macho <- concept_vec(M_norm, labels_macho)
  v_nice  <- concept_vec(M_norm, labels_nice)

  if (any(is.na(v_macho)) || any(is.na(v_nice))) {
    return(tibble())  # skip if no concept representation available
  }

  # cosine to all words
  cos_macho <- cosine_to_all(M_norm, v_macho)
  cos_nice  <- cosine_to_all(M_norm, v_nice)

  tibble(
    decade = d,
    word   = rownames(M_norm),
    cos_macho = cos_macho,
    cos_nice  = cos_nice,
    rel_mac   = cos_macho - cos_nice  # ds4psych: relative association
  )
})

# --- 5) Nearest neighbors & valence time-series --------------
k <- 10  # top-k neighbors per concept per decade

# Top-k by concept
topk_long <- bind_rows(
  results |> group_by(decade) |> slice_max(order_by = cos_macho, n = k) |>
    mutate(concept = "macho", rank = row_number(), score = cos_macho) |>
    select(decade, concept, word, rank, score),

  results |> group_by(decade) |> slice_max(order_by = cos_nice, n = k) |>
    mutate(concept = "nice", rank = row_number(), score = cos_nice) |>
    select(decade, concept, word, rank, score)
) |>
  ungroup()

# Join valence and summarize per decade
valence_ts <- topk_long |>
  left_join(warriner, by = "word") |>
  group_by(decade, concept) |>
  summarize(
    k = n(),
    mean_valence = mean(valence, na.rm = TRUE),
    prop_valence_available = mean(!is.na(valence)),
    .groups = "drop"
  ) |>
  mutate(concept = factor(concept, levels = c("macho","nice")))

# --- 6) Visualize (quick ggplot) -----------------------------
ggplot(valence_ts, aes(decade, mean_valence, group = concept)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ concept, ncol = 1, scales = "free_y") +
  labs(title = "Valence of top-k neighbors over time",
       subtitle = paste0("k = ", k, " (Warriner valence)"),
       x = "Decade", y = "Mean valence of neighbors")

# --- 7) Optional diagnostics: content churn ------------------
# % overlap of top-k neighbors between adjacent decades, by concept
churn <- topk_long |>
  arrange(concept, decade, rank) |>
  group_by(concept) |>
  group_split()

overlap_df <- map_df(churn, function(dfc) {
  split_by_decade <- dfc |>
    group_by(decade) |>
    summarize(words = list(word), .groups = "drop")

  if (nrow(split_by_decade) < 2) return(tibble())
  tibble(
    concept = unique(dfc$concept),
    decade_from = head(split_by_decade$decade, -1),
    decade_to   = tail(split_by_decade$decade, -1),
    jaccard     = map2_dbl(
      head(split_by_decade$words, -1),
      tail(split_by_decade$words, -1),
      ~ { inter <- length(intersect(.x, .y)); union <- length(union(.x, .y)); inter/union }
    )
  )
})

print(overlap_df)

# --- 8) How to query nearest neighbors interactively ---------
# Example: neighbors of "macho" and "nice" in 1950
query_neighbors <- function(decade, query_word, top = 15) {
  emb_path <- file.path(histwords_dir, paste0(decade, ".txt"))
  stopifnot(file.exists(emb_path))
  M <- read_embeddings(emb_path)
  M_norm <- l2_normalize(M)
  if (!query_word %in% rownames(M_norm)) return(tibble())
  sims <- as.numeric(M_norm %*% M_norm[query_word, ])
  tibble(word = rownames(M_norm), cosine = sims) |>
    arrange(desc(cosine)) |>
    slice_head(n = top + 1) |>
    filter(word != query_word)
}

query_neighbors(1950, "macho", top = 15) |> print(n = 16)
query_neighbors(1950, "nice",  top = 15) |> print(n = 16)

# M_norm: L2-normalized embedding matrix with rownames = vocab
phrase_vec <- function(M_norm, phrase_tokens) {
  mats <- lapply(phrase_tokens, function(tk) if (tk %in% rownames(M_norm)) M_norm[tk, , drop=FALSE] else NULL)
  mats <- Filter(Negate(is.null), mats)
  if (length(mats) == 0) return(NA)
  v <- Reduce(`+`, mats) / length(mats)
  # renormalize
  v / sqrt(sum(v^2))
}

# prefer single-token if it exists; else compose
get_concept_vec <- function(M_norm, candidates, fallback_tokens) {
  tok <- candidates[candidates %in% rownames(M_norm)][1]
  if (!is.na(tok)) {
    v <- M_norm[tok, ]
    return(v / sqrt(sum(v^2)))
  } else {
    return(phrase_vec(M_norm, fallback_tokens))
  }
}

# examples:
v_macho_guy <- get_concept_vec(M_norm,
                               candidates = c("macho_guy","tough_guy"),   # try phrase tokens if present
                               fallback_tokens = c("macho","guy"))        # else compose

v_nice_guy  <- get_concept_vec(M_norm,
                               candidates = c("nice_guy","gentleman"),    # single-token if present
                               fallback_tokens = c("nice","guy"))


```
