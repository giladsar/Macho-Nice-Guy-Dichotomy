---
title: "Macho-Nice Guy Dichotomy"
author: "Gilad Sarusi"
format: 
  html:
    toc: true
    number-sections: true
    self-contained: true
execute:
  warning: false
  message: false
editor: visual
---

# HistWords repo

For our purposes, I use the same approach chosen by Tessa E. S. Charlesworth and colleagues—namely, the HistWords repository, which is available on [GitHub](https://github.com/williamleif/histwords?tab=readme-ov-file).

### What's going on?

HistWords is an open-source dataset and toolkit that provides word embeddings for different historical periods, trained on Google Books Ngram (and other corpora). Each decade (1800–1990) has its own 300-dimensional vector space representing the meaning of words in that era.

Think of it as a longitudinal semantic dataset—rather than tracking people over time, we track words across decades. Each vector encodes a word’s position in the shared “meaning space” of its time.

#### What can we do with it?

-   **Inspect meaning:** find the nearest neighbors of a word in any decade.

-   **Compare meanings over time:** compute cosine similarity for the same word across decades to track semantic drift.

-   **Study cultural change:** analyze how a word’s associations (e.g., *macho*, *nice*) evolve by decade.

-   **Compose phrases:** average vectors (e.g., *macho* + *guy*) to represent multi-word concepts.

-   **Align embeddings:** apply sequential Procrustes alignment to make spaces from different decades comparable.

### What are we comparing?

In this project, I adopt an approach similar to Tessa et al. designed to avoid the **alignment problem** inherent in diachronic word embeddings—which makes direct comparison of raw cosine similarities between two embedding spaces (e.g., the 1910s and the 1920s) invalid.

Instead of directly comparing raw cosine similarities of words across decades, I examine **within-decade** associations—for example, comparing how strongly *macho* and *nice* are each associated with the words in a given decade.

The difference between these associations is then converted into a standardized effect size, which serves as the unit of comparison across time. This method lets me trace semantic or attitudinal change without requiring the embedding spaces to be aligned.

-   **Across time (overall)**—I identify the top 10 and top 50 word and trait associates that are most strongly (and relatively) associated with one type of man (and least with the comparison group) by:

    1.  computing cosine similarity of all words and traits to each type in each decade (e.g., the cosine similarities of all traits to *macho man* in 1800). After inspecting distances, I test whether there is a significant difference between the two types—i.e., whether the descriptors are semantically distinct from those of *nice guy*—as a sanity check; and

    2.  calculating the difference in MAC scores; *Macho* (MAC) − *Nice* (MAC) identifies words/traits most associated with *Macho*, and reversing the subtraction captures the opposite pattern. In short, across time, which words and traits are most associated with Macho/Nice Guy?

-   **Across time (by decade)**—I examine the top trait and word associates for each decade:

    1.  compute decade-specific cosine-similarity difference scores and rank words and traits within each decade; and

    2.  compute the average, decade-wise valence of the top associates by averaging ratings of those words’ positivity/negativity in each of the 20 decades.

Nevertheless, following Tessa et al.’s guidance, applying **orthogonal Procrustes** rotation can still be useful as a robustness check or for future extensions involving cross-decade word-trajectory analyses.

::: callout-caution
## [`The following code is written using the ds4psych: Data Science for Psychology: Natural Language workflow (https://ds4psych.com). Therefore, the toolset used here differs from Tessa’s.`]{.smallcaps}
:::

```{r loadPackages, echo = FALSE, message=FALSE}
# ----------------------------------------------------
# Packages
library(tidyverse)
library(vroom)       # fast reading of large txts
library(stringr)
library(purrr)
library(sweater)
library(quanteda.textmodels)
library(quanteda)
library(embedplyr)
library(knitr)
```

```{r Datapaths, include=FALSE}

# --- 0) Paths  -------------------

decades <- seq(1800, 1990, by = 10)      
model_names <- setNames(paste0("eng.all_sgns.",decades), decades) # all English books (Google N-grams)


```

Notably, in the 2000s there was a change in N-gram sampling. Therefore, embeddings using the corpus from the 2000s onward are excluded.

```{r}
## Words norm - for valence computation: 


warriner_path <- "wordstim/allwrdnorms_warriner.csv"          # columns: word, valence, arousal, dominance - will be used as  valence indicates the average valence rating on a 9-point scale from 1 (very positive) to 9 (very negative), obtained from Warriner and colleagues (2013) 

```

As an alternative, we might consider NRC VAD (S. M. Mohammad, 2018), which contains 20,007 words with ratings between 0 and 1 for valence, arousal, and dominance.

```{r}

## List of traits:

traits_path <- "wordstim/traitlist.txt"

# We would latter on create a file storing those words for the code to be more elegant :

nice_guy_words <- c(
  "kind", "gentle", "caring", "sensitive", "considerate", "respectful",
  "supportive", "understanding", "emotional", "warm", "affectionate",
  "polite", "reliable", "faithful", "honest", "loyal", "thoughtful",
  "attentive", "humble", "nice","whipped", "romantic"
)
macho_guy_words <- c(
   "dominant", "assertive", "confident", "tough", "strong", "decisive",
  "bold", "powerful", "competitive", "aggressive", "independent", "unemotional",
  "fearless","alpha", "leader", "charismatic", "commanding", "cocky", "ambitious", "macho"
)

```

### Pay attention!

To ensure that words are embedded in the context of masculine traits, I examine two approaches. The first is to **inject male anchors**. This should help position each word group within a masculine semantic field rather than in a generic trait context (traits that could appear in feminine contexts as well). This is implemented in the *Concepts* chunk.

-   **Cons / watch-outs:**\
    *Anchor dominance risk*—male anchors can outweigh trait terms if the lists are small. Moreover, shared anchors in both groups reduce the distance between group centroids (smaller margins).\
    **Monitor:** anchor share in each centroid (≤30–40%), cosine margin between group means, and Top-N stability across decades (overlap %).

The second approach, implemented in the next *Helpers* chunk, is a **masculinity projection nudge**. I construct a male–female axis and then “masculinize” any term by nudging it toward the male direction before scoring. This gently steers polysemous words toward the male sense without hard-wiring “man” as a single axis for all items.

-   **Cons / watch-outs:**\
    *Axis quality*—if the gender sets are unbalanced or drift strongly by decade, the direction can be noisy.\
    *Leakage*—over-weighting (large *w*) can wash out original semantics.\
    **Monitor:** axis stability across decades (cosine between decade-specific *d*<sub>male</sub>), use small *w* (e.g., 0.1–0.3) and run sensitivity analyses; compare results with and without the tweak.

A comparison of both techniques is needed to validate results and serve as a robustness check.

### Masculinity projection nudge

As noted earlier, to interpret traits within a masculine frame, we apply a Masculinity Nudge: we construct a female–male axis in the embedding space and add a small component of this axis to each construct vector, gently shifting it toward the male semantic field so that subsequent comparisons reflect the words’ masculine sense.

```{r}



# E: numeric matrix (rows = tokens, cols = dims), 

# male_tokens / female_tokens: anchors for the axis (can leave defaults)

 male_anchors <-  c("man","men","male","guy")  #Question: should we add 'boy'? 
 female_anchors <-  c("woman", "women", "female","girl")

 
 build_male_female_axis <- function(embeddings) {
  male_ddr <- predict(embeddings, male_anchors) |> average_embedding()
  female_ddr <- predict(embeddings, female_anchors) |> average_embedding()
  male_ddr - female_ddr
}

 

# DDR-based masculinity function:

# w = nudge strength (0 = no nudge, >0 = push toward masculine direction)
 
# default is set to 1 to inteperate results easily and to make sure that the wors be are assigning is regards to word_GUY not that word in general
 
 
masculinity_nudge <- function(ddr_vec, # Or any dim1:dim300 embedding object
                              E, # The embedding space 
                              w = 1) {
  
  # ensure DDR is a numeric vector
  if (is.matrix(ddr_vec)) ddr_vec <- as.numeric(ddr_vec)
  
  axis <- build_male_female_axis(E)

  
  # apply nudge: shift toward masculine axis 
  ddr_vec_nudged <- ddr_vec + w * axis
  
  ddr_vec_nudged
}





```

The second approach—the **Inject approach**—as noted above, adds male anchor words to the word banks so that the average embedding of each list is drawn toward a masculine context.

```{r Concepts, include=FALSE}
# --- 2) Defining concepts (labels) -----------------------------


# Inject them inside the Macho/Nice Construct 
nice_guy_words_injected <- nice_guy_words |>
  c(male_anchors)

macho_guy_words_injected <- macho_guy_words |>
    c(male_anchors)

```

```{r loadData1, include=FALSE}
# --- 3) Loading Data  ----------------


## -------------  Warriner's valence-----------

#(I'm keeping Arousal and Dominance since it might be useful later)

warriner <- read_csv(warriner_path, show_col_types = FALSE) |>
  transmute(word=tolower(Word),
         #Valence
         valence=V.Mean.Sum,
         # Arousal
         arousal=A.Mean.Sum,
         # Dominance
         dominance = D.Mean.Sum,
         id = as.character(row_number())
         )


## ------------ Lists of traits --------------


trait_list<- read.delim(traits_path, header = FALSE) |>
  # the traits are stored in a column named V1 and I want to use it as a vector 
  pull(V1) 


  # I would fetch only the traits that got crowdslebelling in warriner: 
traits_warriner <- warriner |>
  filter(word %in% trait_list) 
  
```

### ENGall Model

Properties of ENGall_model:

-   n of words: 100,000 (same word list across all decades). the authors only computed embeddings for words that were among the top 100,000 most frequent words over all time (for EngAll)
    -   n of dimensions: 300.
    -   Type of Embedding : SGNS.
    -   Context Window : symmetric context window of 4 words.

```{r}
## ----------- ENGall model -----------

if (file.exists("engall/ENGall_model.RDS")) {
  ENGall_model<-readRDS("engall/ENGall_model.RDS")
}else {
  ENGall_model <- lapply(model_names, load_embeddings, save = FALSE) # list of embedding models
  saveRDS(ENGall_model, 
          file="engall/ENGall_model.RDS")
}
length(ENGall_model)
```

The ENGall model provides embeddings for the 100,000 most frequent words **across** all time. This means words coined later in history will be missing in earlier slices, and some words will be absent in particular decade models. The model is stored as a list of 20 elements, one per decade. In some decade data frames, certain words have no embedding.

let's map those words.

```{r}

#### ---- Check unavailable words by decade ----
n_avwords <- vector()
n_unav<- vector()
prop_avail <- vector()
n_total <- vector()

# Creating a list of available word and unavailable words: 

for (i in seq_along(ENGall_model)) {
  # Count available words as those with V1 != 0
  n_avwords[i] <- sum(ENGall_model[[i]][, 1] != 0)
  n_total[i] <- ENGall_model[[i]]|>
      nrow()  
  n_unav[i]  <- n_total[i] - n_avwords[i]
  prop_avail[i] <- n_avwords[i] / n_total[i]
}


avwords_n_decade_df <- data.frame(
  decade      = decades,
  n_avwords   = n_avwords,
  n_unavwords = n_unav,
  prop_avail  = prop_avail
)

```

```{r availWordsTable}
kable(avwords_n_decade_df |>
        #adding comma
        mutate(across(where(is.numeric),scales::comma)),
      digits = 3,
      caption = "Proportion of Words with Embedding")

```

Out of 100,000 words, each decade has on average 36,347 available words.

Notably, the 1990s slice has \~71% of words available. This does **not** mean “29% are post-1990 words.” Rather, 29% of the top-100k (over 1800–1999) simply lack enough occurrences in the 1990s to train vectors—often because they are earlier-era words that faded out, or because they are rare or orthographically different in that decade.

Later, we will pull valence from Warriner’s list (\~14,000 words), which bounds our effective vocabulary to those items. Because not all Warriner words appear in ENGall, the intersection may be smaller.

Another concern is whether some of the words used to define our constructs are missing from the model.

Let's check!

```{r}

### ---- Check if macho/nice words by decade ------- 
# 1) Availability (token x decade)

avail <- map2_df(ENGall_model, decades, ~
  tibble(
    decade    = .y,
    token     = tolower(rownames(.x)),
    available = .x[, 1] != 0
  )
)

#  Define the sets you care about
sets <- list(
  macho = macho_guy_words,
  nice  = nice_guy_words,
  male = male_anchors
)

wanted <- enframe(sets, name = "set", value = "token") |>
  unnest(token) |>
  mutate(token = tolower(token))

# proportion per decade + list of missing words 
macho_nice_words_by_decade_df <- wanted |>
  left_join(avail, by = "token") |>
  mutate(available = tidyr::replace_na(available, FALSE)) |>
  group_by(set, decade) |>
  summarise(
    n_of_words_examined = n(),
    n_available = sum(available),
    prop_avail    = n_available / n_of_words_examined,
    missing     = list(token[!available]),
    .groups = "drop"
  ) |>
  arrange(set, decade)

```

```{r}

kable(macho_nice_words_by_decade_df,
      caption= "Construct's word presence across time")
```

It seems the words *cocky* and *macho* do not appear in ENGall at all. I therefore omit them to avoid adding noise.

*(Consult with Almog about the unequal lengths of the nice vs. macho lists after omission: length(nice) = 22; length(macho) = 18.)*

```{r}
length(macho_guy_words)
#[1] 20

length(macho_guy_words_injected)

#[1] 24

length(male_anchors)
# [1] 4

macho_guy_words <- setdiff(macho_guy_words,c("cocky", "macho"))
macho_guy_words_injected <- setdiff(macho_guy_words_injected,c("cocky", "macho"))

# Sanity check
length(macho_guy_words)
# [1] 18

length(macho_guy_words_injected)

# [1] 23

# OK I'm sane
    
    
```

# DDR

I will next construct the Macho and Nice Guy semantic contrasts (DDRs). To do so, I will implement a function that iterates over the word-embedding models by decade. When the nudge option is set to TRUE, the function will first construct a masculinity-nudge axis. It will then compute the DDRs for each guy-type construct.

Following Luis’ recommendation, rather than manually computing the difference between Macho and Nice Guy DDRs, I will use the internal anchored method implemented in get_sims. In this specification, a single similarity variable is produced, capturing relative proximity to the Macho versus Nice Guy anchors. Positive values indicate greater semantic association with the Macho prototype, whereas negative values indicate greater association with the Nice Guy prototype.

```{r DDR }

# --- 


decade_scores <- lapply(ENGall_model, function(embeddings){
  embeddings <- embeddings[magnitude(embeddings) != 0,]

  # Nudged approach
  nice_guy_ddr_nudge <- masculinity_nudge(ddr_vec = average_embedding(predict(embeddings,
                                                                              nice_guy_words)),
                                          E = embeddings)
  
  macho_guy_ddr_nudge <- masculinity_nudge(ddr_vec = average_embedding(predict(embeddings, macho_guy_words)),
                                           E =embeddings ) 
  
  # Inject approach
  
  nice_guy_ddr_inject  <- average_embedding(predict(embeddings, nice_guy_words_injected))
  macho_guy_ddr_inject <- average_embedding(predict(embeddings, macho_guy_words_injected))
  
  embeddings |> 
    get_sims(
      list("macho_vs_nice_nudge" = list(pos = macho_guy_ddr_nudge,
                                        neg = nice_guy_ddr_nudge),
           "macho_vs_nice_inject"   = list(pos = macho_guy_ddr_inject,
                                           neg = nice_guy_ddr_inject)
      ),
      method = "anchored"
    )
})

# Make it readable 
names(decade_scores) <- decades




```

## Warriner List

I'll keep **only** the words in the model that have valence scores—i.e., those appearing in Warriner.

*(Warriner contains \~14,000 words with valence. After this examination, I repeat the analysis with the trait list—i.e., restricting to words that appear in both Warriner and the trait list.)*

```{r WarrinerOnly)}
# Starting with empty 20 long list 

warriner_with_scores <- vector("list", length(decade_scores))

for (i in seq_along(decade_scores)) {

  ds <- decade_scores[[i]]

  # --- standardize the token column name to "word" ---
  
  if ("doc_id" %in% names(ds)) {
    ds2 <- ds |> rename(word= doc_id)
  } else {
    stop(
      "Could not find a word column in decade_scores[[i]]. Check names(decade_scores[[i]])."
      )
  }

  # --- join: keep only Warriner words that exist in this decade vocab ---
  warriner_with_scores[[i]] <-
    warriner |>
    dplyr::inner_join(
      ds2 |> dplyr::select(word, macho_vs_nice_nudge, macho_vs_nice_inject),
      by = "word"
    )
}
names(warriner_with_scores) <- decades

warriner_with_scores


#-------- NOTICE! ------

# Before the comparisons I'll omit the words that were used to create the DDR, namely, macho_guy_word and nice_guy_words, to reduce noise .

omit_words <- unique(c(
  macho_guy_words, nice_guy_words,
  macho_guy_words_injected, nice_guy_words_injected
))

warriner_scores_omit <- lapply(warriner_with_scores, function(df) {
  df |> filter(!word %in% omit_words)
})


```

## Attractiveness

In this section, I constructed a decade-specific Attractiveness DDRs and then examine how semantically close the macho and nice representations are to attractiveness across time.

The goal is to quantify how the semantic association between Masochistic and gentle masculinity and attractiveness has shifted across historical decades.

### Word list

We begin by specifying a set of core adjectives that directly express attractiveness (e.g., attractive, beautiful, handsome, sexy, etc.). Two versions are created:

-   Nudge version: uses only the attractiveness descriptors.

-   Inject version: includes the attractiveness descriptors plus the male anchor words, ensuring a stronger masculine reference frame in the embedding space.

This parallels the earlier (Macho/Nice) pipeline for consistency.

```{r}
attractiveness_words <- c(
  "attractive",
  "beautiful",
  "handsome",
  "gorgeous",
  "pretty",
  "sexy",
  "cute",
  "alluring",
  "charming",
  "appealing"
)

# injected
attractiveness_words_injected <- attractiveness_words|>
  c(male_anchors)

```

For each historical decade represented in the ENGall embedding model, we compute an Attractiveness DDR using the same procedure applied previously to Macho and Nice:

**Inject condition**

1.  Retrieve the embeddings of all attractiveness words + male anchors.
2.  Compute their **average embedding**, yielding the *Attractiveness Injected DDR* for that *decade*.

**Nudge condition**

1.  Retrieve the embeddings of the attractiveness adjectives.
2.  Average them to obtain the raw *Attractiveness Nudge DDR*.
3.  Apply the **`masculinity_nudge()`** transformation to align the DDR with the nudge paradigm used for the Macho/Nice DDRs

Each DDR represents a *single semantic vector* summarizing the meaning of attractiveness in that decade.

### Cosine Similarity from Attractiveness DDRs

In the same chulk I will create the DDR for attractiveness and the cos sim from our Macho and Nice DDRS

```{r Cosine from Attractiveness DDR}
cosine_from_attractiveness <- lapply(seq_along(ENGall_model), function(i) {

  embeddings <- ENGall_model[[i]]
  embeddings <- embeddings[magnitude(embeddings) != 0, ]

  # ---------- NUDGE ----------
  nice_ddr_nudge  <- masculinity_nudge(
    ddr_vec = average_embedding(predict(embeddings, nice_guy_words)),
    E = embeddings
  )
  macho_ddr_nudge <- masculinity_nudge(
    ddr_vec = average_embedding(predict(embeddings, macho_guy_words)),
    E = embeddings
  )
  attract_ddr_nudge <- masculinity_nudge(
    ddr_vec = average_embedding(predict(embeddings, attractiveness_words)),
    E = embeddings
  )
  # ---------- DDRs: INJECT ----------
  nice_ddr_inject  <- average_embedding(predict(embeddings, nice_guy_words_injected))
  
  macho_ddr_inject <- average_embedding(predict(embeddings, macho_guy_words_injected))
  
  attract_ddr_inject <- average_embedding(predict(embeddings, attractiveness_words_injected))
  
  # Put DDRs into a tiny "embedding space" (3 rows × 300 dims)
  ddr_space<- rbind(
    #nudge
    macho_ddr_nudge = as.numeric(macho_ddr_nudge),
    nice_ddr_nudge  = as.numeric(nice_ddr_nudge),
    attract_ddr_nudge  = as.numeric(attract_ddr_nudge),
    #inject
    macho_ddr_inject = as.numeric(macho_ddr_inject),
    nice_ddr_inject  = as.numeric(nice_ddr_inject),
    attract_ddr_inject  = as.numeric(attract_ddr_inject)
  )
  ddr_space <- embedplyr::as.embeddings(ddr_space)
  # Cosine of each DDR row to the attractiveness DDR
  
  sims <- ddr_space |>
    get_sims(
      list("macho_vs_nice_nudge" = list(pos = macho_ddr_nudge,
                                        neg = nice_ddr_nudge),
           "macho_vs_nice_inject"   = list(pos = macho_ddr_inject,
                                           neg = nice_ddr_inject)
      ),
      method = "anchored"
    )


  # ---------- Return one tidy row per decade ----------
  
  
  tibble::tibble(
    i_decade = i,
    decade   = decades[i],
    # since the computation is done row by row - I would only extract the one where the anchor is regarded to attractive ddr 
    attract_on_macho_vs_nice_nudge =
      sims$macho_vs_nice_nudge[sims$doc_id == "attract_ddr_nudge"],
    
    attract_on_macho_vs_nice_inject =
      sims$macho_vs_nice_inject[sims$doc_id == "attract_ddr_inject"]
  )
  
  
})

cosine_from_attractiveness <- bind_rows(cosine_from_attractiveness) |> 
  select(-i_decade)

```

```{r AttractivePlot}
# Binding the results into a data frame for plotting


#write a ggplot to show the results
# Nudge plot
nudge_attr_plot <- ggplot(cosine_from_attractiveness, aes(x = decade)) +
  geom_line(aes(y = attract_on_macho_vs_nice_inject, color = "Inject")) +
  geom_line(aes(y = attract_on_macho_vs_nice_nudge, color = "Nudge")) +
  labs(
    title = "Cosine Similarity from Attractiveness DDR",
    x = "Decade",
    y = "Delta (Macho-Nice)",
    color = "Approrach"
  ) +
  theme_minimal() 
  
```

```{r AttractivePlotDDRs}
#| fig.width: 10
#| fig.height: 6
#| out.width: "95%"
#| fig.align: "center"
#| dev: "png"
#| dpi: 300
#| echo: false
#| message: false
#| warning: false


 nudge_attr_plot
 

```

### Analysis
