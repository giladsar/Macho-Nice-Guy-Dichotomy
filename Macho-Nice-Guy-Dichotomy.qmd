---
title: "Macho-Nice Guy Dichotomy"
author: "Gilad Sarusi"
format: html
editor: visual
---
ok
## HisWords repo

For our purpose I would use the same approach that Tessa E. S. Charlesworth et al. chose. Namely,the HisWords repo which is open online in [GitHub](https://github.com/williamleif/histwords?tab=readme-ov-file).

### What's going on?

HistWords is an open-source dataset and toolkit that provides word embeddings for different historical periods, trained on Google Books Ngram (and other corpora). Each decade (1800–1990) has its own 300-dimensional vector space representing the meaning of words in that era.

Think of it as a longitudinal semantic dataset — instead of people measured over time, we have words measured across decades. Each vector encodes a word’s position in the shared “meaning space” of its time.

#### What We Can Do with It?

Inspect meaning: find nearest neighbors of a word in any decade.

Compare meanings over time: compute cosine similarity between the same word across decades to track semantic drift.

Study cultural change: analyze how the associations of a word (e.g., macho, nice) evolve by decade.

Compose phrases: average vectors (e.g., macho + guy) to represent multi-word concepts.

Align embeddings: apply sequential Procrustes alignment to make spaces from different decades comparable.

### What are We Comparng?

In this project, I'll adopt an approach similar to that of Tessa and colleagues, designed to avoid the ***Alignment Problem*** inherent in diachronic word embeddings - which makes comparison of raw cosine similaritie between two embedding spaces (e.g 1910's and 1920's) invalid.

Instead of directly comparing raw cosine similarities of words across decades, I'll examine within-decade associations—for example, comparing how strongly macho and nice are each associated with good in a given decade.

The difference between these associations is then converted into a standardized effect size, which serves as the unit of comparison across time. This method allows me to trace semantic or attitudinal change without requiring the embedding spaces to be aligned.

-   **Overall-Time**- I'll identify the top 10 (and 50) word and trait associates that are most strongly, relatively associated with one type of men (and least associated with a comparison group) by:
    -   <div>

        1.  Computing the MAC of all words and traits to each individual type in each decade (e.g., the MAC of all traits to Macho-man in 1800), as well as to a comparison group (e.g., the MAC of all traits to Nice-man, in 1800).
        2.  Calculating the difference of the MAC scores; Macho(MAC) - Nice(MAC) = the word/traits most associate with Macho and switching the components would be indicative to capture the opposite.
        3.  Averaging these MAC differences across all 20 decades and ranking the words or traits according to their difference scores .

        </div>
-   **Across-Time -** Ihe top trait and word associates separated by each decade were examined:
    -   <div>

        1.  Decade-specific MAC difference scores and rank words and traits within each decade.
        2.  summarize change in the top 10 (and top 50) trait and word associates over decades by counting the number of traits/ words that overlapped in successive decades (e.g., 1800 to 1810, 1810 to 1820) by computing the correlation of MAC scores across time - Higher correlations indicate greater overlap across the two comparison terms.
        3.  Finally, I'll compute average decade-wise valence of the top trait and word associates by averaging ratings of the top words’ positivity/negativity in each of the 20 decades.

        </div>

Nevertheless, following Tessa et al.’s consideration, I note that applying *orthogonal Procrustes* rotation could still be useful as a robustness check or for future extensions involving cross-decade word trajectory analyses.

::: callout-caution
## The following code is written with the workflow of ds4psych: Data Science for Psychology: Natural Language (https://ds4psych.com). Therfore the toolset used in this code differs from tessa's.
:::

## Code

```{r loadPackages, echo = FALSE, message=FALSE}}
# ----------------------------------------------------
# Packages
library(tidyverse)
library(vroom)       # fast reading of large txts
library(stringr)
library(purrr)
library(sweater)
library(quanteda.textmodels)
library(quanteda)
library(text)
library(embedplyr)


```

```{r Datapaths, include=FALSE}

# --- 0) Paths  -------------------
ENGall_dir <- "engall/wordvecsdata_engall.RData"  # from HistWords repo

decades <- seq(1800, 1990, by = 10)          # demo range; noteworthy, in the 2000s' there was a change in the N-Gram sampling. Therefore, embedding using the corpus of 2000's forward were excluded.


## Words norm - for valence computation: 


warriner_path <- "wordstim/allwrdnorms_warriner.csv"          # columns: word, valence, arousal, dominance - will be used as  valence indicates the average valence rating on a 9-point scale from 1 (very positive) to 9 (very negative), obtained from Warriner and colleagues (2013) 

# We might consider an alternative: NRC VAD (S. M. Mohammad, 2018a) contains 20,007 words with ratings between 0 and 1 for valence, arousal and dominance.


## List of traits:

traits_path <- "wordstim/traitlist.txt"

# We would latter on create a file storing those words for the code to be more elegant :

nice_guy_words <- c(
  "kind", "gentle", "caring", "sensitive", "considerate", "respectful",
  "supportive", "understanding", "emotional", "warm", "affectionate",
  "polite", "reliable", "faithful", "honest", "loyal", "thoughtful",
  "attentive", "humble", "nice","whipped", "romantic"
)
macho_guy_words <- c(
   "dominant", "assertive", "confident", "tough", "strong", "decisive",
  "bold", "powerful", "competitive", "aggressive", "independent", "unemotional",
  "fearless","alpha", "leader", "charismatic", "commanding", "cocky", "ambitious", "macho"
)


# ----- Pay attention! ----- 

#in order to make sure that the words are embedding in the context of masculine  traits I'll examine two approach: The first one would be to simply inject  'male anchors'. This should help to position each words groups into a semantic field and direction of a masculinity and not those traits in general, traits that can most definite appear in a feminine context - done in the 'Concepts' chulk. 

# Cons / Watch-outs-  Anchor dominance risk: male anchors can outweigh trait terms if lists are small.more overShared anchors in both groups reduce between-group centroid distance (smaller margins)

#What to Monitor- Anchor share in the centroid (≤30–40%),Cosine margin between group means, Top-N stability across decades (overlap %).

# The Second approach, done in the next chunk of code 'Helpers' would be Masculinity projection tweak.  I will create e an helper that would deal with this problem by creating a male–female axis and then “masculinize” any term, to nudge it to the 'male' direction before scoring. This tweak gently steers polysemous words toward the male sense without hardwiring “man” on one axis that  applies to all items. 
# 
# Cons / Watch-outs: Axis quality: if the gender sets are unbalanced or drift strongly by decade, the direction can be noisy, Leakage: over-weighting (large w) can wash out the original semantics.
# 
# What to Monitor: Axis stability across decades (cosine between decade-specific d_male - the direction of male axis ). Small w (e.g., 0.1–0.3); run sensitivity analyses. Compare results with and without the tweak.

#a comparison is needed to validate the results yields from those techniques and would serve as robustness check 


```

```{r  Helpers}
# --- 1) Small helpers ---------------------------------------

# Read a single decade embedding file into a matrix with rownames = words

read_embeddings <- function(path) {
  # HistWords format: first line header with counts; thereafter: word + 300 dims
  
  raw <- vroom(path, col_names = FALSE, progress = FALSE, delim = " ")
  # Some files include the first line as counts; detect and drop if so
  if (nchar(raw$X1[1]) > 0 && !is.na(as.numeric(raw$X1[1]))) {
    # first line looks numeric -> drop it
    raw <- raw[-1, ]
  }
  words <- raw$X1
  M <- as.matrix(raw[,-1])
  rownames(M) <- words
  storage.mode(M) <- "double"
  M
}




# Masculinity projection tweak 


# E: numeric matrix (rows = tokens, cols = dims), 
#   rownames(E) are tokens - We should make sure that out embedding model's architecture is built in this manner.
# male_tokens / female_tokens: anchors for the axis (can leave defaults)

 male_anchors <-  c("man","men","male","guy")  #Question: should we add 'boy'? 
 female_anchors <-  c("woman", "women", "female","girl")

 
 build_male_female_axis <- function(E, male_tokens, female_tokens) {
  avail <- rownames(E)
  m <- intersect(tolower(male_tokens),   avail)
  f <- intersect(tolower(female_tokens), avail)
  if (length(m) == 0 || length(f) == 0)
    stop("Insufficient anchors to build gender axis.")
  
  male_mu   <- colMeans(E[m, , drop = FALSE])
  female_mu <- colMeans(E[f, , drop = FALSE])
  axis <- male_mu - female_mu
  axis / sqrt(sum(axis * axis)) # we want the gender axis to represent direction only, not magnitude. this normalization step is meant to make the axis to be of a unit length
 }

# DDR-based masculinity function:

# w = nudge strength (0 = no nudge, >0 = push toward masculine direction)
 
# default is set to 1 to inteperate results easily and to make sure that the wors be are assigning is regards to word_GUY not that word in general
 
 
masculinity_nudge <- function(ddr_vec, # Or any dim1:dim300 embedding object
                              E, # The embedding space 
                              male_tokens = male_anchors,
                              female_tokens = female_anchors,
                              w = 1) {
  
  # ensure DDR is a numeric vector
  if (is.matrix(ddr_vec)) ddr_vec <- as.numeric(ddr_vec)
  
  axis <- build_male_female_axis(E, male_tokens, female_tokens)
  ddr_vec <- ddr_vec / sqrt(sum(ddr_vec * ddr_vec))
  
  # apply nudge: shift toward masculine axis 
  ddr_vec_nudged <- ddr_vec + w * axis
  
  #re normalize  - Consult Almog if necessary 
  ddr_vec_nudged <- ddr_vec_nudged / sqrt(sum(ddr_vec_nudged * ddr_vec_nudged))
  
  ddr_vec_nudged
}


# From df to embedding Object function- to fit 'embedplyr' workflow:

to_embeddings <- function(df){
  # keep available rows, ensure rownames are tokens
  stopifnot(is.data.frame(df), !is.null(rownames(df)))
  
  # Keep only available words - 
  keep <- df$V1 != 0
  
  M <- as.matrix(df[keep, , drop = FALSE])
  # give it the right class so embedplyr methods (predict/emb/find_nearest) work
  class(M) <- c("embeddings", class(M))
  attr(M, "normalized") <- FALSE  # optional meta; not required
  
  # crucial: token_index environment (token -> row index) - since the words are stored as rownames
  idx_env <- new.env(parent = emptyenv())
  toks <- rownames(M)
  for (j in seq_along(toks)) assign(toks[j], j, envir = idx_env)
  attr(M, "token_index") <- idx_env

  M
}


```

```{r Concepts, include=FALSE}
# --- 2) Defining concepts (labels) -----------------------------

 male_anchors <-  c("man","men","male","guy","boy")

# "Inject them inside the Macho/Nice directionary 
nice_guy_words_injected <- nice_guy_words |>
  c(male_anchors)

macho_guy_words_injected <- macho_guy_words |>
    c(male_anchors)

```

```{r loadData1, include=FALSE}
# --- 3) Loading Data  ----------------


## -------------  Warriner's valence-----------

#(I'm keeping Arousal and Dominance since it might be useful later)

warriner <- read_csv(warriner_path, show_col_types = FALSE) |>
  transmute(word=tolower(Word),
         #Valence
         valence=V.Mean.Sum,
         # Arousal
         arousal=A.Mean.Sum,
         # Dominance
         dominance = D.Mean.Sum,
         id = as.character(row_number())
         )


## ------------ Lists of traits --------------


trait_list<- read.delim(traits_path, header = FALSE) |>
  # the traits are stored in a column named V1 and I want to use it as a vector 
  pull(V1) 


  # I would fetch only the traits that got crowdslebelling in warriner: 
traits_warriner <- warriner |>
  filter(word %in% trait_list) 
  

## ----------- ENGall model -----------

 load(ENGall_dir)
#the load(ENGall_dir) creates an 'wordvecs.dat' object of which I want to store in more intuituve object name 
ENGall_model <- wordvecs.dat
length(ENGall_model)
#Propoties of ENGall_model:

# n of words: 100000 * 20 decades.  the authors only computed embeddings for words that were among the top 100,000 most frequent words over all time (for EngAll) 

# n of dimensions: 300
# type of embedding : SGNS 
# contex window :  symmetric context window of 4 words



#### ---- Check unavailable words by decade ----
n_avwords <- vector()
n_unav<- vector()
prop_avail <- vector()
n_total <- vector()

# Creating a list of available word and unavailable words: 

for (i in seq_along(ENGall_model)) {
  # Count available words as those with V1 != 0
  n_avwords[i] <- sum(ENGall_model[[i]]$V1 != 0)
  n_total[i] <- ENGall_model[[i]]|>
      nrow()  
  n_unav[i]  <- n_total[i] - n_avwords[i]
  prop_avail[i] <- n_avwords[i] / n_total[i]
}


avwords_n_decade_df <- data.frame(
  decade      = decades,
  n_avwords   = n_avwords,
  n_unavwords = n_unav,
  prop_avail  = prop_avail
)


#  1. outof 10,000 on average we have in each decade 36,347 available words.
#  2. notworthy is that the last slice 1990 have 71% available words, It doesn’t mean “29% are post-1990 neologisms.” It means 29% of the top-100k (over 1800–1999) simply don’t have enough occurrences in the 1990s slice to train vectors—often because they’re earlier-era words that faded out, or because they’re rare/orthographically different in that decade.

#   Noteworthy, I believe that latter on we would fetch the words' valence from
#   warriner's list of 14,000 words, so we would be bound to 14,000 in our word 
#   embedding. given that not all the words in warriner's might not appear in the #   model's coprus we might be left with even smaller number of words. 


### ---- Check if macho/nice words by decade ------- 
# 1) Availability (token x decade)

avail <- map2_df(ENGall_model, decades, ~
  tibble(
    decade    = .y,
    token     = tolower(rownames(.x)),
    available = .x$V1 != 0
  )
)

#  Define the sets you care about
sets <- list(
  macho = macho_guy_words,
  nice  = nice_guy_words,
  male = male_anchors
)

wanted <- enframe(sets, name = "set", value = "token") |>
  unnest(token) |>
  mutate(token = tolower(token))

# proportion per decade + list of missing words 
macho_nice_words_by_decade_df <- wanted |>
  left_join(avail, by = "token") |>
  mutate(available = tidyr::replace_na(available, FALSE)) |>
  group_by(set, decade) |>
  summarise(
    n_of_words_examined = n(),
    n_available = sum(available),
    prop_avail    = n_available / n_of_words_examined,
    missing     = list(token[!available]),
    .groups = "drop"
  ) |>
  arrange(set, decade)


#Summery, it seems that the words "cocky" and "macho" doesnt' appear at all in the corpus of ENGAll therefore I'll omit them to avoid adding noise: 

## consult with Almog regarding the fact that the list of words betwenn nice and macho words list aren't equal length(nice) =22 and length(macho) = 18 (after omition)

length(macho_guy_words)
#[1] 20

length(macho_guy_words_injected)

#[1] 25

length(male_anchors)
# [1] 5

macho_guy_words <- setdiff(macho_guy_words,c("cocky", "macho"))
macho_guy_words_injected <- setdiff(macho_guy_words_injected,c("cocky", "macho"))

# Sanity check
length(macho_guy_words)
# [1] 18

length(macho_guy_words_injected)

# [1] 23

# OK I'm sane
    
    
```

```{r DDR }

# Now I will create the Macho and Nice guy contracts (DDR). 
# in Addition I'll create DDR for valence - namely, a DDR of 'Good' dictionary


# ---- setting the ENGall model to fit 'embedplyr' workflow

# current the Engall[[i]] is of type df while predict() function expects an embedding object that way we would use the to_embedding() helper:




## ----------- Macho DDR (with injection) ---------- 
Macho_DDR_injected_by_decade <- list()

for (i in seq_along(ENGall_model)) {
  
  # Getting the prediction of embedding DDR of the Macho_guy words (with injection) while converting the ENGall model to ebedding object:
  
  Macho_DDR_injected_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), macho_guy_words_injected)|> 
    
    #I'll average the embedding inside the loop since each DDR embedding is computed  on it's on corpus/decade-  noteworthy the averages is done by Google Trillion Word corpus which meant to weight words base on their frequency. 
    
    
    # Consult with Almog - should the weighting be done by our model of choice (ENGall) setting anchor="good"?
  average_embedding()
}

 
# Make it more readable: 

names(Macho_DDR_injected_by_decade) <- decades

## ----------- Nice DDR (with injection) ---------- 

# Follow the same process as before
Nice_DDR_injected_by_decade <- list()

for (i in seq_along(ENGall_model)) {
  
  Nice_DDR_injected_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), nice_guy_words_injected)|> 
  average_embedding()
}

 
# Make it more readable: 

names(Nice_DDR_injected_by_decade) <- decades


#----------- The 'Nudge Appraoch' -------------


## ----------- Macho DDR Nudge  ---------- 


# Follow the same process as before
Macho_DDR_nudge_by_decade <- list()

for (i in seq_along(ENGall_model)) {

  Macho_DDR_nudge_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), macho_guy_words)|>
  average_embedding() |>
    
    # Same as before but now I'll apply the nudge toward the masculine direction
    
    masculinity_nudge(E = ENGall_model[[i]])
}

names(Macho_DDR_nudge_by_decade) <- decades


## ----------- Nice DDR Nudge  ---------- 

Nice_DDR_nudge_by_decade <- list()

for (i in seq_along(ENGall_model)) {

  Nice_DDR_nudge_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), nice_guy_words)|>
  average_embedding() |>
    
    # Same as before but now I'll apply the nudge toward the masculine direction
    
    masculinity_nudge(E = ENGall_model[[i]])
}

names(Nice_DDR_nudge_by_decade) <- decades




```

```{r DDT -> cosin(warriner)}

# Next, I would computed the Cos similarity between the Macho and Nice guy contracts (DDR) and the words in the model THAT HAVe valence - meaning that they appearing in warriner

# Attention: the Warrineir DF contains 14,000 words with valence, after this examination we would do the same with the traitlist - meaning we would use only the words that appears in warriner AND traitlist.


warriner_with_COSINE_from_DDR <- list() 

# Switching to dfm - in order to change to ds4psych workflow to make which word a feature 

# in order to do so lets switch it to corpus : 

warriner_corpus <- warriner |>
  corpus(
    text_field = "word",
     docid_field ="id")


# REMEMBER the corpus those have valence in it ! 
warriner_dfm <- warriner_corpus |>
  tokens() |>
  dfm()

# now that we have dfm we can use get_sims()




for (i in seq_along(ENGall_model)) {

  
  warriner_with_COSINE_from_DDR[[i]] <- warriner_dfm|>
    
    # Getting the embedding of the Warriner words by ENGallL:
    
    textstat_embedding(to_embeddings(ENGall_model[[i]]))  |> 
    bind_cols(docvars(warriner_corpus)) |>
    
    # Getting the Cosine_squish form Macho with nudge (remeber that its a list by decades so add [[i]])
    
    get_sims(
      V1:V300, 
      list(
        macho_nudge     = Macho_DDR_nudge_by_decade[[i]],
        macho_inject       = Macho_DDR_injected_by_decade[[i]],
        nice_nudge      = Nice_DDR_nudge_by_decade[[i]],
        nice_inject       = Nice_DDR_injected_by_decade[[i]]
      ), 
      method = "cosine_squished"
    ) |> 
    
    # adding the word for the results to be readable 
    left_join(warriner |> select(word,id),
              by= c("doc_id" ="id"))
}
names(warriner_with_COSINE_from_DDR) <- decades



#-------- NOTICE! ------

# Before the comparisons I'll omit the words that were used to create the DDR, namely, macho_guy_word and nice_guy_words, to reduce noise .

# a sanity check has reviled that the those words receivned  higher cos sim (DUH!)
warriner_COSINE_DDR_omit<- list()
for (i in seq_along(warriner_with_COSINE_from_DDR)) {
  warriner_COSINE_DDR_omit[[i]]<- warriner_with_COSINE_from_DDR[[i]] |> 
    filter(!word %in% c(macho_guy_words, nice_guy_words))
}
names(warriner_COSINE_DDR_omit) <- decades

#Exporting  files for inspection 
# for (i in seq_along(warriner_with_COSINE_from_DDR)) {
# 
#   write_csv(
#     warriner_COSINE_DDR_omit[[i]],
#     file = paste("csv/",paste(decades[i],"decade.csv"))
#   )
# }

```

```{r Comparisons}
#Helper - Comparison:
compare_DDRs<- function(df) {
  
  df |> 
    mutate(
      # Nudge
      delta_nudge = macho_nudge - nice_nudge,
      # Inject
      delta_inject = macho_inject - nice_inject,
    )
}


# Adding the variables to the list 
warriner_COSINE_DDR_Diff <- list() 
for (i in seq_along(warriner_COSINE_DDR_omit)) {
  warriner_COSINE_DDR_Diff[[i]]<- compare_DDRs(warriner_COSINE_DDR_omit[[i]])
}
names(warriner_COSINE_DDR_Diff) <- decades 

# Make it a long format for it to be easier to work with :


bind_with_decade <- function(dflist) {
  
  tibble(
    decade = names(dflist),
    df     = dflist
  ) |>
    mutate(df = map2(df, decade, ~ .x |>
                       mutate(decade = .y))) |>
    pull(df) |>
    bind_rows()
}

raw_long <- bind_with_decade(warriner_COSINE_DDR_Diff) |>
  mutate(
    
    # Make `decade` an ordered factor (sorted numerically)
    
    decade     = factor(decade, levels = sort(unique(as.integer(decade))) |> as.character())
  ) 

# ---- Q1 ------

#is Overall theres a difference in association across 200 years between macho and nice guy to those list of words? :

tt <- t.test(raw_long$macho_nudge,raw_long$nice_nudge,alternative = "two.sided") 
if(tt$p.value < 0.05) {
  print("Q1 - supports of H1")
} else {
  print("Q1 - supports of H0")
}

compare_nice_macho_words <- function (data, only_traits =F){
  
  #removing NA  by delta_nudge 
  data <- data |> 
    filter(!is.na(delta_nudge)) |>
    
    mutate(doc_id = factor(doc_id))
  # Checking id the comparison is done by traits only or all the words
  if(only_traits){
    data <-   filter(doc_id %in% (traits_warriner |>
                                    select(id) |>
                                    pull()))
  }
  
  # THIS FUNCTION IS AT WORD LEVEL NOT DECADE! 
  
  data |>
    group_by(doc_id) |>
    summarise(
      n = n(),
      # Mean
      mean_delta_nudge = mean(delta_nudge, na.rm =T ),
      mean_delta_inject = mean(delta_inject, na.rm =T ),
      
      # SD
      sd_nudge   = sd(delta_nudge, na.rm =T),
      sd_inject   = sd(delta_inject, na.rm =T),
      
      # SE
      se_nudge         = sd_nudge / sqrt(n),
      se_inject         = sd_inject  / sqrt(n),
      
      # T 
      t_nudge          = ifelse(n > 1,
                                mean_delta_nudge / se_nudge,
                                NA_real_),
      t_inject      = ifelse(n > 1,
                             mean_delta_inject / se_inject,
                             NA_real_),
      
      df         = n - 1L,
      
      # P values
      p_t_nudge        = ifelse(n > 1,
                                2 * pt(-abs(t_nudge), df),
                                NA_real_),
      p_t_inject        = ifelse(n > 1,
                                 2 * pt(-abs(t_inject), df),
                                 NA_real_),
      
      
      #Cohen's D:  mean of Δ divided by SD of Δ
      d_z_nudge        = mean_delta_nudge / sd_nudge,
      d_z_inject       = mean_delta_inject / sd_inject,
      
      # Confidence intervals
      ci_low_nudge      = ifelse(n > 1,
                                 mean_delta_nudge + qt(0.025, df) * se_nudge,
                                 NA_real_),
      ci_high_nudge    = ifelse(n > 1,
                                mean_delta_nudge + qt(0.975, df) * se_nudge,
                                NA_real_),
      ci_low_inject      = ifelse(n > 1,
                                  mean_delta_inject + qt(0.025, df) * se_inject,
                                  NA_real_),
      ci_high_inject     = ifelse(n > 1,
                                  mean_delta_inject  + qt(0.975, df) * se_inject,
                                  NA_real_),
      
      # Significance
      
      sign_nudge = case_when(
        p_t_nudge < 0.001 ~ "***",
        p_t_nudge < 0.01  ~ "**",
        p_t_nudge < 0.05  ~ "*",
        TRUE        ~ "No"
      ),
      sign_inject  = case_when(
        p_t_inject < 0.001 ~ "***",
        p_t_inject < 0.01  ~ "**",
        p_t_inject < 0.05  ~ "*",
        TRUE        ~ "No"
      ),
      .groups = "drop" |>
        left_join(raw_long |> select(doc_id,
                                     word,
                                     valence),
                  by= "doc_id")|>
        # Staying on word level - row = words
        distinct()
    )
}




####----- GENERAL COMPARISON -----


# NOTE: This we aren't not with the function because the grouping here is done by decade and not word:

summ_t_wtnin_dcds_overall <- raw_long|>
  #removing na  by delta_nudge 
  
  filter(!is.na(delta_nudge)) |>
  
  group_by(decade)|>
  summarise(
    n = n(),
    # Mean
    mean_delta_nudge = mean(delta_nudge, na.rm =T ),
    mean_delta_inject = mean(delta_inject, na.rm =T ),
    
    # SD
    sd_nudge   = sd(delta_nudge, na.rm =T),
    sd_inject   = sd(delta_inject, na.rm =T),
    
    # SE
    se_nudge         = sd_nudge / sqrt(n),
    se_inject         = sd_inject  / sqrt(n),
  
    # T 
    t_nudge          = ifelse(n > 1,
                              mean_delta_nudge / se_nudge,
                              NA_real_),
    t_inject      = ifelse(n > 1,
                           mean_delta_inject / se_inject,
                           NA_real_),

    df         = n - 1L,
    
    # P values
    p_t_nudge        = ifelse(n > 1,
                              2 * pt(-abs(t_nudge), df),
                              NA_real_),
    p_t_inject        = ifelse(n > 1,
                              2 * pt(-abs(t_inject), df),
                              NA_real_),
    
    
    #Cohen's D:  mean of Δ divided by SD of Δ
    d_z_nudge        = mean_delta_nudge / sd_nudge,
    d_z_inject       = mean_delta_inject / sd_inject,
    
    # Confidence intervals
    ci_low_nudge      = ifelse(n > 1,
                               mean_delta_nudge + qt(0.025, df) * se_nudge,
                               NA_real_),
    ci_high_nudge    = ifelse(n > 1,
                              mean_delta_nudge + qt(0.975, df) * se_nudge,
                              NA_real_),
    ci_low_inject      = ifelse(n > 1,
                               mean_delta_inject + qt(0.025, df) * se_inject,
                               NA_real_),
    ci_high_inject     = ifelse(n > 1,
                              mean_delta_inject  + qt(0.975, df) * se_inject,
                              NA_real_),
    
    # Significance
    
    sign_nudge = case_when(
      p_t_nudge < 0.001 ~ "***",
      p_t_nudge < 0.01  ~ "**",
      p_t_nudge < 0.05  ~ "*",
      TRUE        ~ "No"
    ),
    sign_inject  = case_when(
      p_t_inject < 0.001 ~ "***",
      p_t_inject < 0.01  ~ "**",
      p_t_inject < 0.05  ~ "*",
      TRUE        ~ "No"
    ),
    .groups = "drop"
  )

summ_t_inject <- summ_t_wtnin_dcds_overall |>
  select(-contains("_nudge"))
# NOT SIGNIFICANT - 1870

summ_t_nudge <- summ_t_wtnin_dcds_overall |>
  select(-contains("_inject"))


# Overall it seems that in the majority of decades across techniques of DDR there has been a significant difference in the association of words to the macho and nice guys- it's not that informative beside the fact that we were able to create two constructs that indeed are representing two different things 

# -------- Q2-----------
# --------- ACROSS DECADES------------ 
# What words or traits are most associate to each DDR in across decades, testing only the significance difference once: 

words_t_summ<- compare_nice_macho_words (raw_long)
  
## ------  Top 20 Words------
### ----- Macho --------------
top_20_words_macho_nudge_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_nudge)
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     

top_20_words_macho_inject_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_inject)
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     

### ----- Nice - top 20 words --------------
top_20_words_nice_nudge_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_nudge
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     

top_20_words_nice_inject_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_inject
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     



# Thoughts: I think we need rto considerate to have an inclusion criteria which only allows words that has been showing in more than +- 5 decades, since we have alot of top 20 words that have only appeared in a limited number of decades: 

table(top_20_words_nice_nudge_overall$n)
# [1] 2 || 3 || 4 || 20 
# [2] 12|| 2 || 1 || 5 
table(top_20_words_macho_nudge_overall$n)
# [1] 2 ||  3 ||  7 
# [2]15 ||  4 ||  1 

## ------  Top 10 traits------


# There's two options to address the traits analysis:
# the first option in to conduct the statistical test on the traits_warriner Df where the SD would be calculated in accordance with the traits delta while the second option would be to filter fro, words_t_summs only the words that are in traits_warriner- meaning the SD would be caculated on the ENTIRE words list. 

# The First option seems in my opinion to be the right one. 

# NOTICE that in warriner_COSINE_DDR_omit we omitted the traits that were used to create our DDRs - so we shouldn't see any wo these words (macho_guy_words and nice_guy_words) in the trait list


traits_t_summ<- compare_nice_macho_words(raw_long,only_traits = T)

#### ------ Macho -------
top_10_traits_macho_nudge_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_nudge)
  ) |>  
  
  # fetching the top 10
  slice_head(n = 10)     


top_10_traits_macho_inject_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_inject)
  ) |>  
  
  # fetching the top 10
  slice_head(n = 10)     

### ----- Nice --------------
top_10_traits_nice_nudge_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_nudge
  ) |>  
  
  # fetching the to 20
  slice_head(n = 10)     

top_10_traits_nice_inject_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_inject
  ) |>  
  
  # fetching the top 10
  slice_head(n = 10)     

# Insights: it seems that the problem of small n()- times that the traits appear in a decade model - is less severe, most of the words appear more then 10 times overall across models

valeace_traits

# --------- WITHIN DECADES------------ 

#----- Q3 ----

# I would run the same test within each decade to identify the top 10 most related traits (and maybe words although they are much more complicated to qualitatively infer something)

# the list of 20 df with deltas: warriner_COSINE_DDR_Diff

summ_t_across_decades <-list()
for (i in seq_along(warriner_COSINE_DDR_Diff)) {
  summ_t_across_decades[[i]]<-compare_nice_macho_words(warriner_COSINE_DDR_Diff[[i]])
}

```

```{r Plots}
```
