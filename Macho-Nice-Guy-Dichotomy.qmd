---
title: "Macho-Nice Guy Dichotomy"
author: "Gilad Sarusi"
format: 
  html:
    toc: true
    number-sections: true
    self-contained: true
execute:
  warning: false
  message: false
editor: visual
---
HistWords repo
==============

For our purposes, I use the same approach chosen by Tessa E. S. Charlesworth and colleagues—namely, the HistWords repository, which is available on [GitHub](https://github.com/williamleif/histwords?tab=readme-ov-file).

### What's going on?

HistWords is an open-source dataset and toolkit that provides word embeddings for different historical periods, trained on Google Books Ngram (and other corpora). Each decade (1800–1990) has its own 300-dimensional vector space representing the meaning of words in that era.

Think of it as a longitudinal semantic dataset—rather than tracking people over time, we track words across decades. Each vector encodes a word’s position in the shared “meaning space” of its time.

#### What can we do with it?

* **Inspect meaning:** find the nearest neighbors of a word in any decade.
    
* **Compare meanings over time:** compute cosine similarity for the same word across decades to track semantic drift.
    
* **Study cultural change:** analyze how a word’s associations (e.g., _macho_, _nice_) evolve by decade.
    
* **Compose phrases:** average vectors (e.g., _macho_ + _guy_) to represent multi-word concepts.
    
* **Align embeddings:** apply sequential Procrustes alignment to make spaces from different decades comparable.
    

### What are we comparing?

In this project, I adopt an approach similar to Tessa et al. designed to avoid the **alignment problem** inherent in diachronic word embeddings—which makes direct comparison of raw cosine similarities between two embedding spaces (e.g., the 1910s and the 1920s) invalid.

Instead of directly comparing raw cosine similarities of words across decades, I examine **within-decade** associations—for example, comparing how strongly _macho_ and _nice_ are each associated with the words in a given decade.

The difference between these associations is then converted into a standardized effect size, which serves as the unit of comparison across time. This method lets me trace semantic or attitudinal change without requiring the embedding spaces to be aligned.

* **Across time (overall)**—I identify the top 10 and top 50 word and trait associates that are most strongly (and relatively) associated with one type of man (and least with the comparison group) by:
    
    1. computing cosine similarity of all words and traits to each type in each decade (e.g., the cosine similarities of all traits to _macho man_ in 1800). After inspecting distances, I test whether there is a significant difference between the two types—i.e., whether the descriptors are semantically distinct from those of _nice guy_—as a sanity check; and
        
    2. calculating the difference in MAC scores; _Macho_ (MAC) − _Nice_ (MAC) identifies words/traits most associated with _Macho_, and reversing the subtraction captures the opposite pattern. In short, across time, which words and traits are most associated with Macho/Nice Guy?
        
* **Across time (by decade)**—I examine the top trait and word associates for each decade:
    
    1. compute decade-specific cosine-similarity difference scores and rank words and traits within each decade; and
        
    2. compute the average, decade-wise valence of the top associates by averaging ratings of those words’ positivity/negativity in each of the 20 decades.
        

Nevertheless, following Tessa et al.’s guidance, applying **orthogonal Procrustes** rotation can still be useful as a robustness check or for future extensions involving cross-decade word-trajectory analyses.

::: callout-caution

[`The following code is written using the ds4psych: Data Science for Psychology: Natural Language workflow (https://ds4psych.com). Therefore, the toolset used here differs from Tessa’s.`]{.smallcaps}
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

:::


```{r loadPackages, echo = FALSE, message=FALSE}
# ----------------------------------------------------
# Packages
library(tidyverse)
library(vroom)       # fast reading of large txts
library(stringr)
library(purrr)
library(sweater)
library(quanteda.textmodels)
library(quanteda)
library(text)
library(embedplyr)
library(knitr)
```

```{r Datapaths, include=FALSE}

# --- 0) Paths  -------------------
ENGall_dir <- "engall/wordvecsdata_engall.RData"  # from HistWords repo

decades <- seq(1800, 1990, by = 10)      


```

Notably, in the 2000s there was a change in N-gram sampling. Therefore, embeddings using the corpus from the 2000s onward are excluded.
```{r}
## Words norm - for valence computation: 


warriner_path <- "wordstim/allwrdnorms_warriner.csv"          # columns: word, valence, arousal, dominance - will be used as  valence indicates the average valence rating on a 9-point scale from 1 (very positive) to 9 (very negative), obtained from Warriner and colleagues (2013) 

```

As an alternative, we might consider NRC VAD (S. M. Mohammad, 2018), which contains 20,007 words with ratings between 0 and 1 for valence, arousal, and dominance.
```{r}

## List of traits:

traits_path <- "wordstim/traitlist.txt"

# We would latter on create a file storing those words for the code to be more elegant :

nice_guy_words <- c(
  "kind", "gentle", "caring", "sensitive", "considerate", "respectful",
  "supportive", "understanding", "emotional", "warm", "affectionate",
  "polite", "reliable", "faithful", "honest", "loyal", "thoughtful",
  "attentive", "humble", "nice","whipped", "romantic"
)
macho_guy_words <- c(
   "dominant", "assertive", "confident", "tough", "strong", "decisive",
  "bold", "powerful", "competitive", "aggressive", "independent", "unemotional",
  "fearless","alpha", "leader", "charismatic", "commanding", "cocky", "ambitious", "macho"
)

```

### Pay attention!

To ensure that words are embedded in the context of masculine traits, I examine two approaches. The first is to **inject male anchors**. This should help position each word group within a masculine semantic field rather than in a generic trait context (traits that could appear in feminine contexts as well). This is implemented in the _Concepts_ chunk.

* **Cons / watch-outs:**  
    _Anchor dominance risk_—male anchors can outweigh trait terms if the lists are small. Moreover, shared anchors in both groups reduce the distance between group centroids (smaller margins).  
    **Monitor:** anchor share in each centroid (≤30–40%), cosine margin between group means, and Top-N stability across decades (overlap %).
    

The second approach, implemented in the next _Helpers_ chunk, is a **masculinity projection nudge**. I construct a male–female axis and then “masculinize” any term by nudging it toward the male direction before scoring. This gently steers polysemous words toward the male sense without hard-wiring “man” as a single axis for all items.

* **Cons / watch-outs:**  
    _Axis quality_—if the gender sets are unbalanced or drift strongly by decade, the direction can be noisy.  
    _Leakage_—over-weighting (large _w_) can wash out original semantics.  
    **Monitor:** axis stability across decades (cosine between decade-specific _d_<sub>male</sub>), use small _w_ (e.g., 0.1–0.3) and run sensitivity analyses; compare results with and without the tweak.
    

A comparison of both techniques is needed to validate results and serve as a robustness check.


```{r  Helpers}
# --- 1) Small helpers ---------------------------------------

# Read a single decade embedding file into a matrix with rownames = words

read_embeddings <- function(path) {
  # HistWords format: first line header with counts; thereafter: word + 300 dims
  
  raw <- vroom(path, col_names = FALSE, progress = FALSE, delim = " ")
  # Some files include the first line as counts; detect and drop if so
  if (nchar(raw$X1[1]) > 0 && !is.na(as.numeric(raw$X1[1]))) {
    # first line looks numeric -> drop it
    raw <- raw[-1, ]
  }
  words <- raw$X1
  M <- as.matrix(raw[,-1])
  rownames(M) <- words
  storage.mode(M) <- "double"
  M
}

```

### Masculinity projection nudge

As noted earlier, to interpret traits within a masculine frame, we apply a Masculinity Nudge: we construct a female–male axis in the embedding space and add a small component of this axis to each construct vector, gently shifting it toward the male semantic field so that subsequent comparisons reflect the words’ masculine sense.
```{r}



# E: numeric matrix (rows = tokens, cols = dims), 
#   rownames(E) are tokens - We should make sure that out embedding model's architecture is built in this manner.
# male_tokens / female_tokens: anchors for the axis (can leave defaults)

 male_anchors <-  c("man","men","male","guy")  #Question: should we add 'boy'? 
 female_anchors <-  c("woman", "women", "female","girl")

 
 build_male_female_axis <- function(E, male_tokens, female_tokens) {
  avail <- rownames(E)
  m <- intersect(tolower(male_tokens),   avail)
  f <- intersect(tolower(female_tokens), avail)
  if (length(m) == 0 || length(f) == 0)
    stop("Insufficient anchors to build gender axis.")
  
  male_mu   <- colMeans(E[m, , drop = FALSE])
  female_mu <- colMeans(E[f, , drop = FALSE])
  axis <- male_mu - female_mu
  axis / sqrt(sum(axis * axis)) # we want the gender axis to represent direction only, not magnitude. this normalization step is meant to make the axis to be of a unit length
 }

# DDR-based masculinity function:

# w = nudge strength (0 = no nudge, >0 = push toward masculine direction)
 
# default is set to 1 to inteperate results easily and to make sure that the wors be are assigning is regards to word_GUY not that word in general
 
 
masculinity_nudge <- function(ddr_vec, # Or any dim1:dim300 embedding object
                              E, # The embedding space 
                              male_tokens = male_anchors,
                              female_tokens = female_anchors,
                              w = 1) {
  
  # ensure DDR is a numeric vector
  if (is.matrix(ddr_vec)) ddr_vec <- as.numeric(ddr_vec)
  
  axis <- build_male_female_axis(E, male_tokens, female_tokens)
  ddr_vec <- ddr_vec / sqrt(sum(ddr_vec * ddr_vec))
  
  # apply nudge: shift toward masculine axis 
  ddr_vec_nudged <- ddr_vec + w * axis
  
  #re normalize  - Consult Almog if necessary 
  ddr_vec_nudged <- ddr_vec_nudged / sqrt(sum(ddr_vec_nudged * ddr_vec_nudged))
  
  ddr_vec_nudged
}


# From df to embedding Object function- to fit 'embedplyr' workflow:

to_embeddings <- function(df){
  # keep available rows, ensure rownames are tokens
  stopifnot(is.data.frame(df), !is.null(rownames(df)))
  
  # Keep only available words - 
  keep <- df$V1 != 0
  
  M <- as.matrix(df[keep, , drop = FALSE])
  # give it the right class so embedplyr methods (predict/emb/find_nearest) work
  class(M) <- c("embeddings", class(M))
  attr(M, "normalized") <- FALSE  # optional meta; not required
  
  # crucial: token_index environment (token -> row index) - since the words are stored as rownames
  idx_env <- new.env(parent = emptyenv())
  toks <- rownames(M)
  for (j in seq_along(toks)) assign(toks[j], j, envir = idx_env)
  attr(M, "token_index") <- idx_env

  M
}


```

The second approach—the **Inject approach**—as noted above, adds male anchor words to the word banks so that the average embedding of each list is drawn toward a masculine context.

```{r Concepts, include=FALSE}
# --- 2) Defining concepts (labels) -----------------------------

 male_anchors <-  c("man","men","male","guy","boy")

# "Inject them inside the Macho/Nice dictionary 
nice_guy_words_injected <- nice_guy_words |>
  c(male_anchors)

macho_guy_words_injected <- macho_guy_words |>
    c(male_anchors)

```

```{r loadData1, include=FALSE}
# --- 3) Loading Data  ----------------


## -------------  Warriner's valence-----------

#(I'm keeping Arousal and Dominance since it might be useful later)

warriner <- read_csv(warriner_path, show_col_types = FALSE) |>
  transmute(word=tolower(Word),
         #Valence
         valence=V.Mean.Sum,
         # Arousal
         arousal=A.Mean.Sum,
         # Dominance
         dominance = D.Mean.Sum,
         id = as.character(row_number())
         )


## ------------ Lists of traits --------------


trait_list<- read.delim(traits_path, header = FALSE) |>
  # the traits are stored in a column named V1 and I want to use it as a vector 
  pull(V1) 


  # I would fetch only the traits that got crowdslebelling in warriner: 
traits_warriner <- warriner |>
  filter(word %in% trait_list) 
  
```

### ENGall Model

Properties of ENGall_model:

-   n of words: 100,000 (same word list across all decades). the authors only computed embeddings for words that were among the top 100,000 most frequent words over all time (for EngAll)
    -   n of dimensions: 300.
    -   Type of Embedding : SGNS.
    -   Context Window : symmetric context window of 4 words.

```{r}
## ----------- ENGall model -----------

 load(ENGall_dir)
#the load(ENGall_dir) creates an 'wordvecs.dat' object of which I want to store in more intuituve object name 
ENGall_model <- wordvecs.dat
length(ENGall_model)

```

The ENGall model provides embeddings for the 100,000 most frequent words **across** all time. This means words coined later in history will be missing in earlier slices, and some words will be absent in particular decade models. The model is stored as a list of 20 elements, one per decade. In some decade data frames, certain words have no embedding.

let's map those words.

```{r}

#### ---- Check unavailable words by decade ----
n_avwords <- vector()
n_unav<- vector()
prop_avail <- vector()
n_total <- vector()

# Creating a list of available word and unavailable words: 

for (i in seq_along(ENGall_model)) {
  # Count available words as those with V1 != 0
  n_avwords[i] <- sum(ENGall_model[[i]]$V1 != 0)
  n_total[i] <- ENGall_model[[i]]|>
      nrow()  
  n_unav[i]  <- n_total[i] - n_avwords[i]
  prop_avail[i] <- n_avwords[i] / n_total[i]
}


avwords_n_decade_df <- data.frame(
  decade      = decades,
  n_avwords   = n_avwords,
  n_unavwords = n_unav,
  prop_avail  = prop_avail
)

```

```{r availWordsTable}
kable(avwords_n_decade_df |>
        #adding comma
        mutate(across(where(is.numeric),scales::comma)),
      digits = 3,
      caption = "Proportion of Words with Embedding")

```

Out of 100,000 words, each decade has on average 36,347 available words.

Notably, the 1990s slice has ~71% of words available. This does **not** mean “29% are post-1990 words.” Rather, 29% of the top-100k (over 1800–1999) simply lack enough occurrences in the 1990s to train vectors—often because they are earlier-era words that faded out, or because they are rare or orthographically different in that decade.

Later, we will pull valence from Warriner’s list (~14,000 words), which bounds our effective vocabulary to those items. Because not all Warriner words appear in ENGall, the intersection may be smaller.

Another concern is whether some of the words used to define our constructs are missing from the model.


Let's check!

```{r}

### ---- Check if macho/nice words by decade ------- 
# 1) Availability (token x decade)

avail <- map2_df(ENGall_model, decades, ~
  tibble(
    decade    = .y,
    token     = tolower(rownames(.x)),
    available = .x$V1 != 0
  )
)

#  Define the sets you care about
sets <- list(
  macho = macho_guy_words,
  nice  = nice_guy_words,
  male = male_anchors
)

wanted <- enframe(sets, name = "set", value = "token") |>
  unnest(token) |>
  mutate(token = tolower(token))

# proportion per decade + list of missing words 
macho_nice_words_by_decade_df <- wanted |>
  left_join(avail, by = "token") |>
  mutate(available = tidyr::replace_na(available, FALSE)) |>
  group_by(set, decade) |>
  summarise(
    n_of_words_examined = n(),
    n_available = sum(available),
    prop_avail    = n_available / n_of_words_examined,
    missing     = list(token[!available]),
    .groups = "drop"
  ) |>
  arrange(set, decade)

```

```{r}

kable(macho_nice_words_by_decade_df,
      caption= "Construct's word presence across time")
```


It seems the words _cocky_ and _macho_ do not appear in ENGall at all. I therefore omit them to avoid adding noise.

_(Consult with Almog about the unequal lengths of the nice vs. macho lists after omission: length(nice) = 22; length(macho) = 18.)_


```{r}
length(macho_guy_words)
#[1] 20

length(macho_guy_words_injected)

#[1] 25

length(male_anchors)
# [1] 5

macho_guy_words <- setdiff(macho_guy_words,c("cocky", "macho"))
macho_guy_words_injected <- setdiff(macho_guy_words_injected,c("cocky", "macho"))

# Sanity check
length(macho_guy_words)
# [1] 18

length(macho_guy_words_injected)

# [1] 23

# OK I'm sane
    
    
```

# DDR

Now I will create the Macho and Nice guy contracts (DDR). Currently the Engall\[\[i\]\] (the model of a specific decade) is of type df while predict() function expects an embedding object that way we would use the to_embedding() helper:

```{r DDR }


# ---- setting the ENGall model to fit 'embedplyr' workflow





## ----------- Macho DDR (with injection) ---------- 
Macho_DDR_injected_by_decade <- list()

for (i in seq_along(ENGall_model)) {
  
  # Getting the prediction of embedding DDR of the Macho_guy words (with injection) while converting the ENGall model to ebedding object:
  
  Macho_DDR_injected_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), macho_guy_words_injected)|> 
    
    #I'll average the embedding inside the loop since each DDR embedding is computed  on it's on corpus/decade-  noteworthy the averages is done by Google Trillion Word corpus which meant to weight words base on their frequency. 
    
    
    # Consult with Almog - should the weighting be done by our model of choice (ENGall) setting anchor="good"?
  average_embedding()
}

 
# Make it more readable: 

names(Macho_DDR_injected_by_decade) <- decades

## ----------- Nice DDR (with injection) ---------- 

# Follow the same process as before
Nice_DDR_injected_by_decade <- list()

for (i in seq_along(ENGall_model)) {
  
  Nice_DDR_injected_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), nice_guy_words_injected)|> 
  average_embedding()
}

 
# Make it more readable: 

names(Nice_DDR_injected_by_decade) <- decades


#----------- The 'Nudge Appraoch' -------------


## ----------- Macho DDR Nudge  ---------- 


# Follow the same process as before
Macho_DDR_nudge_by_decade <- list()

for (i in seq_along(ENGall_model)) {

  Macho_DDR_nudge_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), macho_guy_words)|>
  average_embedding() |>
    
    # Same as before but now I'll apply the nudge toward the masculine direction
    
    masculinity_nudge(E = ENGall_model[[i]])
}

names(Macho_DDR_nudge_by_decade) <- decades


## ----------- Nice DDR Nudge  ---------- 

Nice_DDR_nudge_by_decade <- list()

for (i in seq_along(ENGall_model)) {

  Nice_DDR_nudge_by_decade[[i]] <- predict(to_embeddings(ENGall_model[[i]]), nice_guy_words)|>
  average_embedding() |>
    
    # Same as before but now I'll apply the nudge toward the masculine direction
    
    masculinity_nudge(E = ENGall_model[[i]])
}

names(Nice_DDR_nudge_by_decade) <- decades




```

## Cos Sims

DDRs are just points in semantic space. Next, I compute the cosine similarity of each model word to these DDRs.

Specifically, I compute cosine similarity between the Macho and Nice DDRs and **only** the words in the model that have valence scores—i.e., those appearing in Warriner.

_(Warriner contains ~14,000 words with valence. After this examination, I repeat the analysis with the trait list—i.e., restricting to words that appear in both Warriner and the trait list.)_

```{r DDT -> cosin(warriner)}



warriner_with_COSINE_from_DDR <- list() 

# Switching to dfm - in order to change to ds4psych workflow to make which word a feature 

# in order to do so lets switch it to corpus : 

warriner_corpus <- warriner |>
  corpus(
    text_field = "word",
     docid_field ="id")


# REMEMBER the corpus those have valence in it ! 
warriner_dfm <- warriner_corpus |>
  tokens() |>
  dfm()

# now that we have dfm we can use get_sims()



for (i in seq_along(ENGall_model)) {

  
  warriner_with_COSINE_from_DDR[[i]] <- warriner_dfm|>
    
    # Getting the embedding of the Warriner words by ENGallL:
    
    textstat_embedding(to_embeddings(ENGall_model[[i]]))  |> 
    bind_cols(docvars(warriner_corpus)) |>
    
    # Getting the Cosine_squish form Macho with nudge (remeber that its a list by decades so add [[i]])
    
    get_sims(
      V1:V300, 
      list(
        macho_nudge     = Macho_DDR_nudge_by_decade[[i]],
        macho_inject       = Macho_DDR_injected_by_decade[[i]],
        nice_nudge      = Nice_DDR_nudge_by_decade[[i]],
        nice_inject       = Nice_DDR_injected_by_decade[[i]]
      ), 
      method = "cosine_squished"
    ) |> 
    
    # adding the word for the results to be readable 
    left_join(warriner |> select(word,id),
              by= c("doc_id" ="id"))
}
names(warriner_with_COSINE_from_DDR) <- decades



#-------- NOTICE! ------

# Before the comparisons I'll omit the words that were used to create the DDR, namely, macho_guy_word and nice_guy_words, to reduce noise .

# a sanity check has reviled that the those words received higher cos sim (DUH!)
warriner_COSINE_DDR_omit<- list()
for (i in seq_along(warriner_with_COSINE_from_DDR)) {
  warriner_COSINE_DDR_omit[[i]]<- warriner_with_COSINE_from_DDR[[i]] |> 
    filter(!word %in% c(macho_guy_words, nice_guy_words))
}
names(warriner_COSINE_DDR_omit) <- decades

#Exporting  files for inspection 
# for (i in seq_along(warriner_with_COSINE_from_DDR)) {
# 
#   write_csv(
#     warriner_COSINE_DDR_omit[[i]],
#     file = paste("csv/",paste(decades[i],"decade.csv"))
#   )
# }

```

## Deltas

```{r Comparisons}
#Helper - Comparison:
compare_DDRs<- function(df) {
  
  df |> 
    mutate(
      # Nudge
      delta_nudge = macho_nudge - nice_nudge,
      # Inject
      delta_inject = macho_inject - nice_inject,
    )
}


# Adding the variables to the list 
warriner_COSINE_DDR_Diff <- list() 
for (i in seq_along(warriner_COSINE_DDR_omit)) {
  warriner_COSINE_DDR_Diff[[i]]<- compare_DDRs(warriner_COSINE_DDR_omit[[i]])
}
names(warriner_COSINE_DDR_Diff) <- decades 

# Make it a long format for it to be easier to work with :


bind_with_decade <- function(dflist) {
  
  tibble(
    decade = names(dflist),
    df     = dflist
  ) |>
    mutate(df = map2(df, decade, ~ .x |>
                       mutate(decade = .y))) |>
    pull(df) |>
    bind_rows()
}

raw_long <- bind_with_decade(warriner_COSINE_DDR_Diff) |>
  mutate(
    
    # Make `decade` an ordered factor (sorted numerically)
    
    decade     = factor(decade, levels = sort(unique(as.integer(decade))) |> as.character())
  ) 
```

### ---- Q1 ------

**Is there, overall, a difference in association across 200 years between Macho and Nice Guy for these word lists?**

```{r}
tt <- t.test(raw_long$macho_nudge,raw_long$nice_nudge,alternative = "two.sided") 
if(tt$p.value < 0.05) {
  print("Q1 - supports of H1")
} else {
  print("Q1 - supports of H0")
}

compare_nice_macho_words <- function (data, only_traits =F){
  
  #removing NA  by delta_nudge 
  data <- data |> 
    filter(!is.na(delta_nudge)) |>
    
    mutate(doc_id = factor(doc_id))
  # Checking id the comparison is done by traits only or all the words
  if(only_traits){
    data <-  data |> filter(doc_id %in% (traits_warriner |>
                                    select(id) |>
                                    pull()))
  }
  

  data  |>
     # THIS FUNCTION IS AT WORD LEVEL NOT DECADE! 

    group_by(doc_id) |>
    
    summarise(
      n = n(),
      # Mean
      mean_delta_nudge = mean(delta_nudge, na.rm =T ),
      mean_delta_inject = mean(delta_inject, na.rm =T ),
      
      # SD
      sd_nudge   = sd(delta_nudge, na.rm =T),
      sd_inject   = sd(delta_inject, na.rm =T),
      
      # SE
      se_nudge         = sd_nudge / sqrt(n),
      se_inject         = sd_inject  / sqrt(n),
      
      # T 
      t_nudge          = ifelse(n > 1,
                                mean_delta_nudge / se_nudge,
                                NA_real_),
      t_inject      = ifelse(n > 1,
                             mean_delta_inject / se_inject,
                             NA_real_),
      
      df         = n - 1L,
      
      # P values
      p_t_nudge        = ifelse(n > 1,
                                2 * pt(-abs(t_nudge), df),
                                NA_real_),
      p_t_inject        = ifelse(n > 1,
                                 2 * pt(-abs(t_inject), df),
                                 NA_real_),
      
      
      #Cohen's D:  mean of Δ divided by SD of Δ
      d_z_nudge        = mean_delta_nudge / sd_nudge,
      d_z_inject       = mean_delta_inject / sd_inject,
      
      # Confidence intervals
      ci_low_nudge      = ifelse(n > 1,
                                 mean_delta_nudge + qt(0.025, df) * se_nudge,
                                 NA_real_),
      ci_high_nudge    = ifelse(n > 1,
                                mean_delta_nudge + qt(0.975, df) * se_nudge,
                                NA_real_),
      ci_low_inject      = ifelse(n > 1,
                                  mean_delta_inject + qt(0.025, df) * se_inject,
                                  NA_real_),
      ci_high_inject     = ifelse(n > 1,
                                  mean_delta_inject  + qt(0.975, df) * se_inject,
                                  NA_real_),
      
      # Significance
      
      sign_nudge = case_when(
        p_t_nudge < 0.001 ~ "***",
        p_t_nudge < 0.01  ~ "**",
        p_t_nudge < 0.05  ~ "*",
        TRUE        ~ "No"
      ),
      sign_inject  = case_when(
        p_t_inject < 0.001 ~ "***",
        p_t_inject < 0.01  ~ "**",
        p_t_inject < 0.05  ~ "*",
        TRUE        ~ "No"
      ),
      .groups = "drop"
      ) |>
        left_join(raw_long |> select(doc_id,
                                     word,
                                     valence),
                  by= "doc_id")|>
        # Staying on word level - row = words
        distinct()
    
}






####----- GENERAL COMPARISON -----


# NOTE: This we aren't not with the function because the grouping here is done by decade and not word:

summ_t_wtnin_dcds_overall <- raw_long|>
  #removing na  by delta_nudge 
  
  filter(!is.na(delta_nudge)) |>
  
  group_by(decade)|>
  summarise(
    n = n(),
    # Mean
    mean_delta_nudge = mean(delta_nudge, na.rm =T ),
    mean_delta_inject = mean(delta_inject, na.rm =T ),
    
    # SD
    sd_nudge   = sd(delta_nudge, na.rm =T),
    sd_inject   = sd(delta_inject, na.rm =T),
    
    # SE
    se_nudge         = sd_nudge / sqrt(n),
    se_inject         = sd_inject  / sqrt(n),
  
    # T 
    t_nudge          = ifelse(n > 1,
                              mean_delta_nudge / se_nudge,
                              NA_real_),
    t_inject      = ifelse(n > 1,
                           mean_delta_inject / se_inject,
                           NA_real_),

    df         = n - 1L,
    
    # P values
    p_t_nudge        = ifelse(n > 1,
                              2 * pt(-abs(t_nudge), df),
                              NA_real_),
    p_t_inject        = ifelse(n > 1,
                              2 * pt(-abs(t_inject), df),
                              NA_real_),
    
    
    #Cohen's D:  mean of Δ divided by SD of Δ
    d_z_nudge        = mean_delta_nudge / sd_nudge,
    d_z_inject       = mean_delta_inject / sd_inject,
    
    # Confidence intervals
    ci_low_nudge      = ifelse(n > 1,
                               mean_delta_nudge + qt(0.025, df) * se_nudge,
                               NA_real_),
    ci_high_nudge    = ifelse(n > 1,
                              mean_delta_nudge + qt(0.975, df) * se_nudge,
                              NA_real_),
    ci_low_inject      = ifelse(n > 1,
                               mean_delta_inject + qt(0.025, df) * se_inject,
                               NA_real_),
    ci_high_inject     = ifelse(n > 1,
                              mean_delta_inject  + qt(0.975, df) * se_inject,
                              NA_real_),
    
    # Significance
    
    sign_nudge = case_when(
      p_t_nudge < 0.001 ~ "***",
      p_t_nudge < 0.01  ~ "**",
      p_t_nudge < 0.05  ~ "*",
      TRUE        ~ "No"
    ),
    sign_inject  = case_when(
      p_t_inject < 0.001 ~ "***",
      p_t_inject < 0.01  ~ "**",
      p_t_inject < 0.05  ~ "*",
      TRUE        ~ "No"
    ),
    .groups = "drop"
  )

summ_t_inject <- summ_t_wtnin_dcds_overall |>
  select(-contains("_nudge"))
# NOT SIGNIFICANT - 1870

summ_t_nudge <- summ_t_wtnin_dcds_overall |>
  select(-contains("_inject"))

# NOT SIGNIFICANT - 1980


```

```{r GeneralComparison}
kable (summ_t_wtnin_dcds_overall |> select(decade,
                                        sign_nudge,
                                        sign_inject,
                                        p_t_nudge,
                                        p_t_inject),
       caption = "Significance test of the constructs throught the decades")
```

Overall, in most decades and across both DDR techniques, there is a significant difference in the association of words with _Macho_ versus _Nice_. This mainly serves as a validation that our two constructs capture distinct semantics.

![A win is a Win!](media/win.gif){fig-align="center" width="60%"}

### ---- Q2-------

**Across all decades, which words or traits are most associated with each DDR (testing significance once overall)?**

```{r}
words_t_summ<- compare_nice_macho_words (raw_long)
  
## ------  Top 20 Words------
### ----- Macho --------------
top_20_words_macho_nudge_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_nudge)
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     




top_20_words_macho_inject_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_inject)
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     

### ----- Nice  --------------
top_20_words_nice_nudge_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_nudge
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     

top_20_words_nice_inject_overall <- words_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_inject
  ) |>  
  
  # fetching the to 20
  slice_head(n = 20)     

```

```{r WordsOverall, echo=FALSE}

top_20_words_nice_nudge_overall


nice_table_all_words<- data.frame(
  cohenD_inject = top_20_words_nice_inject_overall$d_z_inject,
  words_inject = top_20_words_nice_inject_overall$word,
  cohenD_nudge = top_20_words_nice_nudge_overall$d_z_nudge,
  word_nudge = top_20_words_nice_nudge_overall$word,
  rank= c(1:10)

)
nice_all_words_table<- kable(
  nice_table_all,
  caption = "Top 10 Nice (vs. Macho) Words",
  digits = 3
)

macho_table_all_words<- data.frame(
  cohenD_inject = top_20_words_macho_inject_overall$d_z_inject,
  words_inject = top_20_words_macho_inject_overall$word,
  cohenD_nudge = top_20_words_macho_nudge_overall$d_z_nudge,
  word_nudge = top_20_words_macho_nudge_overall$word,
  rank= c(1:10)

)
macho_table_all_words<- kable(
  macho_table_all,
  caption = "Top 10 Macho (vs. Nice) Words",
  digits = 3
)

```

::::: columns
::: {.column width="50%"}
```{r}
macho_table_all_words
```
:::

::: {.column width="50%"}
```{r}
nice_table_all_words
```
:::
:::::
**Thought.** We should consider an inclusion criterion that retains only words appearing in ≥ ~5 decades, since many “Top-20” items occur in only a few decades.

```{r thoughtAboutInclusion}
table(top_20_words_nice_nudge_overall$n)
# [1] 2 || 3 || 4 || 20 
# [2] 12|| 2 || 1 || 5 
table(top_20_words_macho_nudge_overall$n)
# [1] 2 ||  3 ||  7 
# [2]15 ||  4 ||  1 

```

For traits, there are two options:

1. Run the statistical test on the `traits_warriner` data frame so that the SD is calculated from trait-level deltas; or
    
2. Filter `words_t_summ` to only those in `traits_warriner`, which computes SD over the **entire** word list.
    

Option (1) seems preferable.

_Note:_ In `warriner_COSINE_DDR_omit` we removed the terms used to create the DDRs, so none of those (_macho_guy_words_ / _nice_guy_words_) should appear in the trait list.

```{r}
## ------  Top 10 traits------




traits_t_summ<- compare_nice_macho_words(raw_long,only_traits = T)

#### ------ Macho -------
top_10_traits_macho_nudge_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_nudge)
  ) |>  
  
  # fetching the top 10
  slice_head(n = 10)    |> 
  mutate(rank = row_number())


top_10_traits_macho_inject_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    desc(d_z_inject)
  ) |>  
  
  # fetching the top 10
  slice_head(n = 10)        |> 
  mutate(rank = row_number())

### ----- Nice --------------
top_10_traits_nice_nudge_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_nudge == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_nudge
  ) |>  
  
  # fetching the to 20
  slice_head(n = 10)       |> 
  mutate(rank = row_number()) 

top_10_traits_nice_inject_overall <- traits_t_summ |> 
  # Sign only
  filter(!sign_inject == "No") |> 
  # arranging to but the standardized delta across decades 
  arrange(
    # Notice this Time I want to get the lowest (minus) since delta is macho minus nice negative value indicate stronger association toward nice 
    d_z_inject
  ) |>  
  
  # fetching the top 10
  slice_head(n = 10)        |> 
  mutate(rank = row_number())

```


**Insight.** The “small _n_” problem (traits appearing in only a few decade models) is less severe here; most traits appear more than 10 times overall across models.
 
 
```{r}


### ---- Tabling-----

nice_table_all<- data.frame(
  cohenD_inject = top_10_traits_nice_inject_overall$d_z_inject,
  words_inject = top_10_traits_nice_inject_overall$word,
  cohenD_nudge = top_10_traits_nice_nudge_overall$d_z_nudge,
  word_nudge = top_10_traits_nice_nudge_overall$word,
  rank= c(1:10)

)
nice_all_traits_table<- kable(
  nice_table_all,
  caption = "Top 10 Nice (vs. Macho) Traits",
  digits = 3
)

macho_table_all<- data.frame(
  cohenD_inject = top_10_traits_macho_inject_overall$d_z_inject,
  words_inject = top_10_traits_macho_inject_overall$word,
  cohenD_nudge = top_10_traits_macho_nudge_overall$d_z_nudge,
  word_nudge = top_10_traits_macho_nudge_overall$word,
  rank= c(1:10)

)
macho_table_all_table<- kable(
  macho_table_all,
  caption = "Top 10 Macho (vs. Nice) Traits",
  digits = 3
)

```

::::: columns
::: {.column width="50%"}
```{r}
macho_table_all_table
```
:::

::: {.column width="50%"}
```{r}
nice_all_traits_table
```
:::
:::::



### ---- Q3 ----

**Over time, does the valence of the words/traits most associated with one construct (vs. the other) change?**


For each decade, I identify the Top-10 most related words and compute their mean valence.

To make interpretation more intuitive, I rescale valence from the original range to −4…+4, where negative values indicate negative valence and positive values indicate, well, positive valence.
```{r}
# --------- WITHIN DECADES------------ 

#----- Q3 ----


get_top_valence <- function(data,
                            decade,
                            is.nudge= F,
                            only_traits = F,
                            macho_vs = T) {
  # Words or Traits ? 
  if(only_traits){
    data <-  data |> filter(doc_id %in% (traits_warriner |>
                                           select(id) |>
                                           pull()))
  }
  # Nudge or Inject
  col <- if (is.nudge) "delta_nudge" else "delta_inject"
  
  
  # If macho_vs is False meaning we want the words that are more associated to nice (vs macho) therefore assenting order of delta since delta is macho MINUS nice 
  dir <- if (macho_vs) -1 else 1
  data <- data[order(dir * data[[col]]), ]
  data  |>
    slice_head(n = 10) |>
    summarise(
      mean_valence = mean(valence, na.rm = TRUE),
      decade = as.numeric(decade)
    )
}




# the list of 20 df with deltas: warriner_COSINE_DDR_Diff

### ------- Macho ------
#  Words Inject

valence_words_macho_decades_inject<-  map2_dfr(
  warriner_COSINE_DDR_Diff, decades,
  ~ get_top_valence(.x,
                    decade = .y)
)|>
  mutate(mean_valence =  mean_valence -4)

  # Words Nudge

valence_words_macho_decades_nudge <- map2_dfr(
  warriner_COSINE_DDR_Diff,
  decades,
  ~ get_top_valence(.x,
                    decade = .y,
                    is.nudge = TRUE)
) |>
  mutate(mean_valence =  mean_valence -4)

  # Traits Inject
valence_traits_macho_decades_inject <- map2_dfr(
  warriner_COSINE_DDR_Diff,
  decades,
  ~ get_top_valence(.x,
                    decade = .y,
                    only_traits = TRUE)
) |>
  mutate(mean_valence =  mean_valence -4)

  
   # Traits nudge

valence_traits_macho_decades_nudge <- map2_dfr(
  warriner_COSINE_DDR_Diff,
  decades,
  ~ get_top_valence(.x,
                    decade = .y,
                    only_traits = TRUE,
                    is.nudge = TRUE)
) |>
  mutate(mean_valence =  mean_valence -4)
  
## -------- Nice 
#  Words Inject

valence_words_nice_decades_inject<-  map2_dfr(
  warriner_COSINE_DDR_Diff, decades,
  ~ get_top_valence(.x,
                    decade = .y,
                    macho_vs = F)
) |>
  mutate(mean_valence =  mean_valence -4)

# Words Nudge
valence_words_nice_decades_nudge <- map2_dfr(
  warriner_COSINE_DDR_Diff,
  decades,
  ~ get_top_valence(.x,
                    decade = .y,
                    is.nudge = TRUE,
                    macho_vs = F)
) |>
  mutate(mean_valence =  mean_valence -4)

# Traits Inject
valence_traits_nice_decades_inject <- map2_dfr(
  warriner_COSINE_DDR_Diff,
  decades,
  ~ get_top_valence(.x,
                    decade = .y,
                    only_traits = TRUE,
                    macho_vs = F)
) |>
  mutate(mean_valence =  mean_valence -4)

# Traits nudge

valence_traits_nice_decades_nudge <- map2_dfr(
  warriner_COSINE_DDR_Diff,
  decades,
  ~ get_top_valence(.x,
                    decade = .y,
                    only_traits = TRUE,
                    is.nudge = TRUE,
                    macho_vs = F)
) |>
  mutate(mean_valence =  mean_valence -4)










# ----- Modeling Macho ----

# Word inject - Macho 
lm_macho_words_inject <- lm(mean_valence ~ decade, data = valence_words_macho_decades_inject)

# Word nudge - Macho 
lm_macho_words_nudge <- lm(mean_valence ~ decade, data = valence_words_macho_decades_nudge)

# Trait inject - Macho 
lm_macho_traits_inject <- lm(mean_valence ~ decade, data = valence_traits_macho_decades_inject)

# Trait nudge - Macho 
lm_macho_traits_nudge <- lm(mean_valence ~ decade, data = valence_traits_macho_decades_nudge)



# --------- Modeling Nice -------- 

# Word inject - Nice 
lm_nice_words_inject <- lm(mean_valence ~ decade, data = valence_words_nice_decades_inject)

# Word nudge - Nice
lm_nice_words_nudge <- lm(mean_valence ~ decade, data = valence_words_nice_decades_nudge)

# Trait inject - Nice 
lm_nice_traits_inject <- lm(mean_valence ~ decade, data = valence_traits_nice_decades_inject)

# Trait nudge -Nice 
lm_nice_traits_nudge <- lm(mean_valence ~ decade, data = valence_traits_nice_decades_nudge)



```


```{r Plots, echo = FALSE}
sig_subtitle <- function(df) {
  mod <- lm(mean_valence ~ decade, data = df)
  b <- coef(mod)[2]
  p <- summary(mod)$coefficients[2, 4]
  sig <- ifelse(p < 0.05, "*", "n.s.")
  sprintf("b = %.2f, p = %.3f (%s)", b, p, sig)
}
# Example: Create 4 ggplots (replace with your real ones)
p1 <- ggplot(valence_words_macho_decades_inject, aes(decade, mean_valence)) +
  geom_line(color = "#f8766d") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Macho - Words Inject",
      subtitle =sig_subtitle(valence_words_macho_decades_inject)
) +
  theme_minimal(base_size = 7)

p2 <- ggplot(valence_words_macho_decades_nudge, aes(decade, mean_valence)) +
  geom_line(color = "#00bfc4") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Macho - Words Nudge",
      subtitle =sig_subtitle(valence_words_macho_decades_nudge)
      ) +
  theme_minimal(base_size = 7)

p3 <- ggplot(valence_words_nice_decades_inject, aes(decade, mean_valence)) +
  geom_line(color = "#7cae00") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Nice - Words Inject",
       subtitle =sig_subtitle(valence_words_nice_decades_inject)
  ) +
  theme_minimal(base_size = 7)

p4 <- ggplot(valence_words_nice_decades_nudge, aes(decade, mean_valence)) +
  geom_line(color = "#c77cff") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Nice - Words Nudge",
       subtitle =sig_subtitle(valence_words_nice_decades_nudge)
  ) +
  theme_minimal(base_size = 7)

# Traits: 

p5 <- ggplot(valence_traits_macho_decades_inject, aes(decade, mean_valence)) +
  geom_line(color = "#f8766d") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Macho - Words Inject",
       subtitle=sig_subtitle(valence_traits_macho_decades_inject)
) +
  theme_minimal(base_size = 7)

p6 <- ggplot(valence_traits_macho_decades_nudge, aes(decade, mean_valence)) +
  geom_line(color = "#00bfc4") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Macho - Words Nudge",
       subtitle=sig_subtitle(valence_traits_macho_decades_nudge)) +
  theme_minimal(base_size = 7)

p7 <- ggplot(valence_traits_nice_decades_inject, aes(decade, mean_valence)) +
  geom_line(color = "#7cae00") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Nice - Words Inject",
       subtitle=sig_subtitle(valence_traits_nice_decades_inject)) +
  theme_minimal(base_size = 7)

p8 <- ggplot(valence_traits_nice_decades_nudge, aes(decade, mean_valence)) +
  geom_line(color = "#c77cff") + 
  geom_smooth(method = "lm", se = T) +
  labs(title = "Nice - Words Nudge",
        subtitle=sig_subtitle(valence_traits_nice_decades_nudge)) +
  theme_minimal(base_size = 7)
```
![Voila!!](media/presentation.gif){fig-align="center" width="60%"}

```{r FinalPlotWords, warning=FALSE, fig.width=12, fig.align='center', fig.fullwidth=TRUE, echo=FALSE}
# ---- Combine 4 plots in a 2×2 grid ----
library(patchwork) 
(p1 | p2) /
(p3 | p4) +
  plot_annotation(
    title = "Valence of Top 10 Words Across Decades",
    subtitle = "Linear trends by condition (Inject vs Nudge)"
  )
```

```{r FinalPlotTraits, warning=FALSE, fig.width=12, fig.align='center', fig.fullwidth=TRUE,echo = FALSE}
# ---- Combine 4 plots in a 2×2 grid ----
library(patchwork) 
(p5 | p6) /
(p7 | p8) +
  plot_annotation(
    title = "Valence of Top 10 Traits Across Decades",
    subtitle = "Linear trends by condition (Inject vs Nudge)"
  )
```
